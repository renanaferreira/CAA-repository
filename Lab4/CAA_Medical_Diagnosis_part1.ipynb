{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZYK-0rin5x7"
   },
   "source": [
    "### LAB: Chest X-Ray Images for Medical Diagnosis\n",
    "\n",
    "** Objectives: ** Prepare data for Building a Deep Learning model to discriminate between 14 different pathologies from chest X-ray images. \n",
    " In particular, you will:\n",
    "- Pre-process and prepare a real-world X-ray dataset\n",
    "- Learn a technique to handle class imbalance\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/xray-header-image.png\" style=\"width:650px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Je3yV0Wnn5x8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import warnings\n",
    "#warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "#warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "#high-level interface for drawing attractive and informative \n",
    "#statistical graphics\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "\n",
    "# util provides locally defined utility functions required for this lab work.\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PMDCWQRn5yA"
   },
   "source": [
    "###  1. Load Chest X-ray Images: Data Exploration and Image Preprocessing \n",
    "\n",
    "**Objectives**: Become familiar with Chest X-ray data set, that is a small part of the images in the  [ChestX-ray8 dataset](https://arxiv.org/abs/1705.02315) which contains 108,948 frontal-view X-ray images of 32,717 patients. \n",
    "\n",
    "The first step for any Machine Learning (ML) project is to explore the data.\n",
    "\n",
    "- Each image in the data set contains labels for 14 different pathological conditions, that can be used by physicians to diagnose different diseases. \n",
    "- We will use this data to develop a binary classification model to predict 'positive' (pathology is present, label 1) or 'negative' (pathology is not present, label 0) for each of the 14 labeled pathologies. \n",
    "- The dataset consists of  422 images stored in folder **nih/images-small/**. \n",
    "- The dataset includes 3 CSV (Comma-Separated Values) files with the labels for each X-ray: \n",
    "\n",
    "1. `nih/train-small.csv`: training images.\n",
    "1. `nih/valid-small.csv`: validation images.\n",
    "1. `nih/test.csv`:testing images . \n",
    "\n",
    "#### Several meanings of word  'class'\n",
    "The word **'class'** is used in multiple ways is this lab. \n",
    "- We sometimes refer to each of the 14 pathological conditions that are labeled in our dataset as a class. \n",
    "- But for each of those pathologies we are attempting to predict whether a certain condition is present (i.e. positive result) or absent (i.e. negative result). \n",
    "    - These two possible labels of 'positive' or 'negative' (or the numerical equivalent of 1 or 0) are also typically referred to as classes. \n",
    "- Moreover, we also use the term in reference to software code 'classes' such as `ImageDataGenerator`.\n",
    "\n",
    "As long as you are aware of all this though, it should not cause you any confusion as the term 'class' is usually clear from the context in which it is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "5JRSHB7i0t_6",
    "outputId": "69830050-af47-4ebc-946d-d411d0cbdf5b"
   },
   "outputs": [],
   "source": [
    "# Read the csv file containing training data with pandas library \n",
    "train_df = ?\n",
    "\n",
    "# Print the first few rows of train_df => dictionary with several keys \n",
    "? \n",
    "\n",
    "#What are the keys ?\n",
    "\n",
    "#What are the dimensions (rows, columns) of train_df and their meaning ? \n",
    "?\n",
    "\n",
    "# Do the same for validation and testing data\n",
    "\n",
    "valid_df = ?\n",
    "\n",
    "test_df = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrDoMlsun5yE"
   },
   "outputs": [],
   "source": [
    "# 14 pathological conditions \n",
    "\n",
    "labels = ['Cardiomegaly', \n",
    "          'Emphysema', \n",
    "          'Effusion', \n",
    "          'Hernia', \n",
    "          'Infiltration', \n",
    "          'Mass', \n",
    "          'Nodule', \n",
    "          'Atelectasis',\n",
    "          'Pneumothorax',\n",
    "          'Pleural_Thickening', \n",
    "          'Pneumonia', \n",
    "          'Fibrosis', \n",
    "          'Edema', \n",
    "          'Consolidation']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data labels\n",
    "The csv file has 16 columns: \"Image\" column with the names of the chest x-ray images, \"PatientId\" column with the patient ID and 14 other (binary) columns filled with 1 or 0 to identify if the pathology is present (1) or not (0) in this x-ray image.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the # of positive labels (1's) for each condition (pathology).\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are supposed to see that data is highly unbalanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Visualization - Display some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imag = []\n",
    "imag.append('00008270_015.png')\n",
    "imag.append('00000116_034.png')\n",
    "imag.append(\"00000359_010.png\")\n",
    "imag.append(\"00000121_008.png\")\n",
    "imag.append('00000003_001.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the image directory\n",
    "IMAGE_DIR = \"nih/images-small/\"\n",
    "\n",
    "print('Display some images')\n",
    "\n",
    "# Adjust the size of the images\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    img = plt.imread(os.path.join(IMAGE_DIR, imag[i]))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate a single image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first image from imag\n",
    "sample_img = imag[0]\n",
    "\n",
    "#  Image dimension ? \n",
    "#Answer: 1024 px width, 1024 px height, 1 color (gray) channel\n",
    "\n",
    "?\n",
    "\n",
    "# Pixel max, min, mean values, standard deviation \n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate pixel value distribution\n",
    "Plot the distribution of pixel values in the image you selected above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    " ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBWZ5l4ln5yH"
   },
   "source": [
    "###  3. Preparing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPjuZHPpn5yH"
   },
   "source": [
    "- We will use now [ImageDataGenerator](https://keras.io/preprocessing/image/) class from Keras framework, which allows to build a \"generator\" for images specified in a dataframe. \n",
    "- This class also provides support for basic data augmentation such as random horizontal flipping of images.\n",
    "- We also use the generator to normalize the values in each batch (train, validate, test sets) so that their mean is $0$ and their standard deviation is 1. In other words, the generator will replace each pixel value in the image with a new value calculated by subtracting the mean and dividing by the standard deviation.  This will facilitate model training by standardizing the input distribution. \n",
    "\n",
    "$$\\frac{x_i - \\mu}{\\sigma}$$\n",
    "\n",
    "- The generator also converts a single channel X-ray images (gray-scale) to a 3-channel format by repeating the values in the image across all channels, because the pre-trained model that we'll use requires 3-channel inputs.\n",
    "\n",
    "The implemented generator below will\n",
    "1. Normalize the mean and standard deviation of the data\n",
    "3. Shuffle the input after each epoch.\n",
    "4. Set the image size to be 320px by 320px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAgVGOAju8pX"
   },
   "outputs": [],
   "source": [
    "def get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, \n",
    "    batch_size=8, seed=1, target_w = 320, target_h = 320):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return generator for training set, normalizing using batch statistics.\n",
    "\n",
    "    Args:\n",
    "      train_df (dataframe): dataframe specifying training data.\n",
    "      image_dir (str): directory where image files are held.\n",
    "      x_col (str): name of column in df that holds filenames.\n",
    "      y_cols (list): list of strings that hold y labels for images.\n",
    "      batch_size (int): images per batch to be fed into model during training.\n",
    "      seed (int): random seed.\n",
    "      target_w (int): final width of input images.\n",
    "      target_h (int): final height of input images.\n",
    "    \n",
    "    Returns:\n",
    "        train_generator (DataFrameIterator): that transforms the entire training set\n",
    "    \"\"\"        \n",
    "    print(\"Train images:\")\n",
    "    # normalize images\n",
    "    image_generator = ImageDataGenerator(\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization= True)\n",
    "    \n",
    "    # flow from directory with specified batch size\n",
    "    # and target image size\n",
    "    generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpRXR-3_u7cl"
   },
   "source": [
    "Now we build a new generator to transform (normalize) the validation and testing data. \n",
    "We can't use the same generator as for the training data because the test data has to be normalized using the statistics computed from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UtWEAfAnrhMq"
   },
   "outputs": [],
   "source": [
    "def get_test_and_valid_generator(valid_df, test_df, train_df, image_dir,\n",
    "    x_col, y_cols, sample_size=100, batch_size=8, seed=1, \n",
    "    target_w = 320, target_h = 320):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return generator for validation set and test test set using \n",
    "    normalization statistics from training set.\n",
    "\n",
    "    Args:\n",
    "      valid_df (dataframe): dataframe specifying validation data.\n",
    "      test_df (dataframe): dataframe specifying test data.\n",
    "      train_df (dataframe): dataframe specifying training data.\n",
    "      image_dir (str): directory where image files are held.\n",
    "      x_col (str): name of column in df that holds filenames.\n",
    "      y_cols (list): list of strings that hold y labels for images.\n",
    "      sample_size (int): size of sample to use for normalization \n",
    "      statistics.\n",
    "      batch_size (int): images per batch to be fed into model during \n",
    "      training.\n",
    "      seed (int): random seed.\n",
    "      target_w (int): final width of input images.\n",
    "      target_h (int): final height of input images.\n",
    "    \n",
    "    Returns:\n",
    "        test_generator (DataFrameIterator) and valid_generator: \n",
    "        iterators over test set and validation set respectively\n",
    "    \"\"\"\n",
    "     # Generator of train images\n",
    "    print(\"Train images:\")\n",
    "    # get generator to sample dataset\n",
    "    raw_train_generator = ImageDataGenerator().flow_from_dataframe(\n",
    "        dataframe=train_df, \n",
    "        directory=IMAGE_DIR, \n",
    "        x_col=\"Image\", \n",
    "        y_col=labels, \n",
    "        class_mode=\"raw\", \n",
    "        batch_size=sample_size, \n",
    "        shuffle=True, \n",
    "        target_size=(target_w, target_h))\n",
    "    \n",
    "    # get data sample (1 image) \n",
    "    batch = raw_train_generator.next()\n",
    "    data_sample = batch[0]\n",
    "\n",
    "    # use 1 sample (1 image) to fit mean and std for test set generator\n",
    "    image_generator = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization= True)\n",
    "    \n",
    "    # fit generator to sample from training data\n",
    "    image_generator.fit(data_sample)\n",
    "\n",
    "    # Generator of validation images\n",
    "    print(\"Validation images:\")\n",
    "    valid_generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=valid_df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "\n",
    "    # Generator of test images\n",
    "    print(\"Test images:\")\n",
    "    test_generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=test_df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "    return valid_generator, test_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga4RZN5On5yL"
   },
   "source": [
    "Apply the generator function to preprocess the training, validation and test data\n",
    "\n",
    "**Note:** The train_df csv file has a list of 1000 images, however in the directory we only have 296 training images, therefore it prints a warning message. and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "rNE3HWRbn5yL",
    "outputId": "4c6b1c25-a33d-42e0-f442-40971ca52a3f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the generators to prepare the training, validation and test data\n",
    "\n",
    "train_generator = ?\n",
    "\n",
    "valid_generator, test_generator= ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYtXacDgn5yN"
   },
   "source": [
    "Let's see what the generator gives to the model by `__getitem__(index)` function and compare with the original image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "Jh77vpN-n5yO",
    "outputId": "c4e68e79-e8f2-4bb9-8909-072c9dd2f805"
   },
   "outputs": [],
   "source": [
    "generated_image, label = train_generator.__getitem__(0)\n",
    "\n",
    "# What is the shape of the generated_image ? \n",
    "?\n",
    "# What is the meaning of each dimension ?\n",
    "\n",
    "# What is the pixel values range (max, min) of the normalized images ?\n",
    "?\n",
    "\n",
    "# What is the Mean pixel value and the standard deviation ?\n",
    "\n",
    "?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first image from generated_image (normalized image)\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first image from imag (original image)\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms of the original and the normalized images and get a similar figure.\n",
    "\n",
    "<img src=\"images/f1.png\" style=\"width:300px;height:200px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of the distribution of the pixels\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9WBMpRxcDMgp"
   },
   "source": [
    "### 4. Addressing Class Imbalance\n",
    "One of the challenges with medical datasets is the large class imbalance. Let's plot the frequency of each of the labels in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the class frequency\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "sns.barplot(labels, np.mean(train_generator.labels, axis=0), color='b')\n",
    "#sns.barplot(np.mean(train_generator.labels, axis=0), labels, color='b')\n",
    "plt.title(\"Frequency of Each Class\", fontsize=10)\n",
    "plt.ylabel('Pathology', fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# What is the dimension and the meaning of train_generator.labels ?\n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3nHRd9p9n5yU"
   },
   "source": [
    "We can see from this plot that the presence of different pathologies (classes) varies significantly. (The same trend happens in the full dataset as well.) \n",
    "* The `Hernia` pathology has the greatest imbalance with the proportion of positive training cases being about 0.2%. \n",
    "* Even the `Infiltration` pathology, has about 15.5% of the training cases labelled positive.\n",
    "\n",
    "If we use a normal cross-entropy loss function with this highly unbalanced dataset, the algorithm will be incentivized to prioritize the majority class (i.e negative in our case), since it contributes more to the loss. \n",
    "\n",
    "#### Impact of class imbalance on loss function\n",
    "\n",
    " We will use a cross-entropy loss for each pathology. The normal cross-entropy loss from the $i^{th}$ training example is:\n",
    "\n",
    "$$\\mathcal{L}_{cross-entropy}(x_i) = -(y_i \\log(f(x_i)) + (1-y_i) \\log(1-f(x_i))),$$\n",
    "\n",
    "where $x_i$ are the input features and $y_i$ is the label, and $f(x_i)$ is the output of the model, i.e. the probability that it is positive (1) or negative (0). \n",
    "\n",
    "For any training case, either $y_i=0$ or $y_i=1$ , so only one term contributes to the loss (the other term is multiplied by zero). \n",
    "\n",
    "The average cross-entropy loss over the entire training set $\\mathcal{D}$ of size $N$ is as follows: \n",
    "\n",
    "$$\\mathcal{L}_{cross-entropy}(\\mathcal{D}) = - \\frac{1}{N}\\big( \\sum_{\\text{positive examples}} \\log (f(x_i)) + \\sum_{\\text{negative examples}} \\log(1-f(x_i)) \\big).$$\n",
    "\n",
    "We can see that if there is a large imbalance with very few positive training cases, then the loss will be dominated by the negative class. Summing the contribution over all the training cases for each class (i.e. pathological condition), we see that the contribution of each class (i.e. positive or negative) is: \n",
    "\n",
    "$$freq_{p} = \\frac{\\text{number of positive examples}}{N} $$\n",
    "\n",
    "$$\\text{and}$$\n",
    "\n",
    "$$freq_{n} = \\frac{\\text{number of negative examples}}{N}.$$\n",
    "\n",
    "\n",
    "### Computing Class Frequencies\n",
    "Complete the function below to calculate these frequences for each label in the dataset.\n",
    "\n",
    "Use numpy.sum(a, axis= ?), and choose the axis (0 or 1) </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TpDGeY2cChYD"
   },
   "outputs": [],
   "source": [
    "def compute_class_freqs(labels):\n",
    "    \"\"\"\n",
    "    Compute positive and negative frequences for each class.\n",
    "\n",
    "    Args:\n",
    "        labels (np.array): matrix of labels, size (num_examples, \n",
    "        num_classes)\n",
    "        \n",
    "    Returns:\n",
    "positive_frequencies (np.array): array of positive frequences for each\n",
    "                                class, size (num_classes)\n",
    "negative_frequencies (np.array): array of negative frequences for each\n",
    "                                class, size (num_classes)\n",
    "    \"\"\"\n",
    "    \n",
    "    # total number of patients (rows)\n",
    "    N = ?\n",
    "    \n",
    "    positive_frequencies = ?\n",
    "    negative_frequencies = ?\n",
    "\n",
    "    return positive_frequencies, negative_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iye-sQoOFG37"
   },
   "source": [
    "Compute the frequencies for our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LoxM5jQ0E30D"
   },
   "outputs": [],
   "source": [
    "freq_pos, freq_neg = ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gsJIDPTZn5yW"
   },
   "source": [
    "Let's visualize these two contribution ratios next to each other for each of the pathologies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "IqnNCu4In5yW",
    "outputId": "245f1a6b-b292-4c6d-a583-c6924bc61f31",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": \n",
    "                     freq_pos})\n",
    "\n",
    "data = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": \n",
    "    v} for l,v in enumerate(freq_neg)], ignore_index=True)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "f = sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2uvttCM8n5yY"
   },
   "source": [
    "As we see in the above plot, the contributions of positive cases is significantly lower than that of the negative ones. However, we want the contributions to be equal. One way of doing this is by multiplying each example from each class by a class-specific weight factor, $w_{pos}$ and $w_{neg}$, so that the overall contribution of each class is the same. \n",
    "\n",
    "\n",
    "$$pos\\_contribution=w_{pos} \\times freq_{p} = w_{neg} \\times freq_{n} =neg\\_contribution$$\n",
    "\n",
    "$$w_{pos} = freq_{neg}$$\n",
    "$$w_{neg} = freq_{pos}$$\n",
    "\n",
    "This way, we will be balancing the contribution of positive and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zs3_Rgwwn5yZ"
   },
   "outputs": [],
   "source": [
    "pos_w = ?\n",
    "neg_w = ?\n",
    "pos_contribution = ?\n",
    "neg_contribution = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygNZmdyun5ya"
   },
   "source": [
    "Verify that now the contribution of the positive and negative labels is equal. It is expected to get a similar plot as the one below. \n",
    "\n",
    "<img src=\"images/f_bal_contr.png\" style=\"width:300px;height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "id": "LPfSFrxjn5yb",
    "outputId": "a4b6354f-ab39-4623-d44b-90cfd9b28506",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9xgoEkpn5yc"
   },
   "source": [
    "As the above figure shows, by applying these weightings the positive and negative labels within each class would have the same aggregate contribution to the loss function. \n",
    "After computing the weights, the final weighted loss for each training case will be \n",
    "\n",
    "$$\\mathcal{L}_{cross-entropy}^{w}(x) = - (w_{p} y \\log(f(x)) + w_{n}(1-y) \\log( 1 - f(x) ) ).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Weighted Loss\n",
    "The function below computes the average weighted loss for all training examples (batch data). For the multi-class loss, the average loss for each individual class is computed. The small value, $\\epsilon$, is added to the predicted values before taking their logs. This is to avoid a numerical error that would otherwise occur if the predicted value happens to be zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPIBVAasn5yd"
   },
   "outputs": [],
   "source": [
    "def get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Return weighted loss function given negative weights and \n",
    "    positive weights.\n",
    "\n",
    "    Args:\n",
    "      pos_weights (np.array): array of positive weights for each class, size (num_classes)\n",
    "      neg_weights (np.array): array of negative weights for each class, size (num_classes)\n",
    "      epsilon: to avoid numerical errors if the predicted value = 0.\n",
    "    \n",
    "Returns:\n",
    "      weighted_loss (function): weighted loss function\n",
    "    \"\"\"\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Return weighted loss value. \n",
    "\n",
    "Args:\n",
    " y_true (Tensor): Tensor of true labels, size is \n",
    " (num_examples, num_classes)\n",
    " y_pred (Tensor): Tensor of predicted labels, size is (num_examples, \n",
    " num_classes)\n",
    " \n",
    "Returns:\n",
    "    loss (Float): overall scalar loss summed across all classes\n",
    "        \"\"\"\n",
    "        # initialize loss to zero\n",
    "        loss = 0.0\n",
    "        \n",
    "        for i in range(len(pos_weights)):\n",
    "# for each class, add average weighted loss for that class\n",
    "#K.mean, K.log are Keras functions to calculate the mean and the log.\n",
    "            \n",
    "            loss_pos = -1 * K.mean(pos_weights[i] * y_true[:, i] * K.log(y_pred[:, i] + epsilon))\n",
    "            loss_neg = -1 * K.mean(neg_weights[i] * (1 - y_true[:, i]) * K.log(1 - y_pred[:, i] + epsilon))\n",
    "            \n",
    "            loss += loss_pos + loss_neg \n",
    "        return loss\n",
    "    \n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "G5aZAlVbn5yz"
   ],
   "include_colab_link": true,
   "name": "C1M2_Assignment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "coursera": {
   "schema_names": [
    "AI4MC1-1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
