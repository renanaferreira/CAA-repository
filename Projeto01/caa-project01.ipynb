{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport warnings\nwarnings.filterwarnings('ignore',category=FutureWarning)\nwarnings.filterwarnings('ignore',category=DeprecationWarning)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom termcolor import colored\nimport os\nimport seaborn as sns\n\nfrom sklearn.metrics import classification_report,confusion_matrix, ConfusionMatrixDisplay\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as tfl\n\nimport tensorflow_datasets as tfds\n\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.layers import concatenate, Conv2DTranspose\nfrom tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\nfrom keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Resizing, Rescaling\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom tensorflow.keras.applications import resnet50, vgg16, mobilenet_v2, MobileNetV2\nfrom keras.applications.imagenet_utils import preprocess_input","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data\n# https://towardsdatascience.com/how-to-split-a-tensorflow-dataset-into-train-validation-and-test-sets-526c8dd29438\ndef get_dataset_partitions_tf(dataset, train_split=0.6, val_split=0.2, test_split=0.2, seed=12):\n    assert (train_split + test_split + val_split) == 1\n    \n    size = int(dataset.cardinality().numpy())\n    \n    # Specify seed to always have the same split distribution between runs\n    dataset = dataset.shuffle(size, seed=seed)\n    \n    train_size = int(train_split * size)\n    val_size = int(val_split * size)\n    \n    train_ds = dataset.take(train_size)   \n    val_ds = dataset.skip(train_size).take(val_size)\n    test_ds = dataset.skip(train_size).skip(val_size)\n    \n    return train_ds, val_ds, test_ds\n\ndef to_categorical(y, num_classes):\n    y = tf.one_hot(tf.cast(y, tf.int32), num_classes)\n    y = tf.cast(y, tf.float32)\n    return y\n\ndef is_categorical(dataset):\n    for _, y in dataset.take(1):\n        output = y.numpy()[0]\n        break\n    if(len(output.shape) == 0):\n        return False\n    else:\n        return True\n    \ndef show_image(element, class_names):\n    x = elem[0]\n    y = elem[1]\n    image = x[0].astype(\"uint8\")\n    idx = y[0]\n    if(is_categorical(dataset)):\n        idx = np.argmax(idx)\n    plt.imshow(image)\n    plt.title(class_names[idx])\n    plt.show()    \n\ndef visualize_dataset(dataset, class_names, title):\n    # visualize\n    categorical = is_categorical(dataset)\n    plt.figure(figsize=(10, 10))\n    elems = list(dataset.as_numpy_iterator())\n    for i in range(9):\n        image = elems[i][0][0]\n        label = elems[i][1][0]\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(image.astype(\"uint8\"))\n        idx = label\n        if categorical:\n            idx = np.argmax(idx)                \n        plt.title(class_names[idx])\n        plt.axis(\"off\")\n            \ndef class_distribution(dataset, class_names):\n    class_values = []\n    categorical = is_categorical(dataset)\n    for element in dataset.as_numpy_iterator():\n        for i in range(len(element[1])):\n            y = element[1][i]\n            if categorical:\n                y = np.argmax(y)\n            class_values.append(class_names[int(y)])\n\n    class_n, frequency = np.unique(np.array(class_values), return_counts=True)\n    df = pd.DataFrame(frequency, class_n, columns=[\"count\"])\n    return df\n\n# frequency bar chart\ndef show_bar_chart(dataset, class_names, title):\n    df = class_distribution(dataset, class_names)\n    df.plot.bar()\n    plt.title(title)\n    plt.show()\n\n\ndef evaluate(model, test_dataset):\n    pred_y = model.predict(test_dataset)\n    pred_y = np.argmax(pred_y, axis=1)\n    \n    true_y = np.asarray(list(test_dataset.map(lambda x,y: y).unbatch().as_numpy_iterator()))\n    if(is_categorical(test_dataset)):\n        true_y = np.argmax(true_y, axis=1)\n    \n    #get confusion matrix\n    cmatrix = confusion_matrix(true_y,pred_y)\n    # get report\n    report = classification_report(true_y,pred_y)\n    \n    return cmatrix, report \n    \ndef show_evaluation(history, conf_matrix, report):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Loss')\n    plt.ylabel('value')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    \n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Accuracy')\n    plt.ylabel('value')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    \n    graph = ConfusionMatrixDisplay(conf_matrix)\n    graph.plot()\n    plt.show()\n    print(report)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set Image pre-processing parameters\nAUTOTUNE = tf.data.experimental.AUTOTUNE  #AUTOTUNE= -1 ???\nIMG_SIZE = (300, 300)\nSHUFFLE=True\nSEED=42\nLABEL_MODE = \"int\"\nCOLOR_MODE = \"rgb\"\nVAL_SPLIT = 0.2\nTEST_SPLIT = 0.2\n\ndirectory = \"/kaggle/input/garbage-classification/Garbage classification/Garbage classification/\"\ndataset = image_dataset_from_directory(directory,\n                                       batch_size=1,\n                                       shuffle=SHUFFLE,\n                                       image_size=IMG_SIZE,\n                                       label_mode=LABEL_MODE,\n                                       color_mode=COLOR_MODE,\n                                       seed=SEED)\n\ntrain_dataset, validation_dataset, test_dataset = get_dataset_partitions_tf(dataset)\n\nCLASS_NAMES = dataset.class_names\nNUM_CLASSES = len(CLASS_NAMES)\n\nDEVELOPMENT = False\nif DEVELOPMENT:\n    seed = 12\n    train_dataset.shuffle(train_dataset.cardinality(),seed=seed)\n    train_dataset = train_dataset.take(480)\n    validation_dataset.shuffle(validation_dataset.cardinality(),seed=seed)\n    validation_dataset = validation_dataset.take(160)\n    test_dataset.shuffle(test_dataset.cardinality(),seed=seed)\n    test_dataset = test_dataset.take(160)","metadata":{"_kg_hide-input":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"{CLASS_NAMES}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"sparse categorical\")\nprint(f\"categorical? {is_categorical(dataset)}\")\nfor elem in dataset.as_numpy_iterator():\n    print(elem[0].shape)\n    show_image(elem, CLASS_NAMES)\n    break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_dataset(dataset, CLASS_NAMES, \"Normal dataset\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"visualize_dataset(train_dataset, CLASS_NAMES, \"Training dataset\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# frequency in train dataset\nshow_bar_chart(train_dataset, CLASS_NAMES, f\"Training data: Frequency\")\nshow_bar_chart(validation_dataset, CLASS_NAMES, f\"Validation data: Frequency\")\nshow_bar_chart(test_dataset, CLASS_NAMES, f\"Test data: Frequency\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_augmenter():\n    '''\n    Create a Sequential model composed of 2 layers\n    Returns:\n        tf.keras.Sequential\n    '''\n\n    data_augmentation = tf.keras.Sequential()\n    data_augmentation.add(RandomFlip(\"horizontal\"))\n    data_augmentation.add(RandomRotation(0.2))\n\n    return data_augmentation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train models\nWe will train different models to set which ones fit our dataset better, basically using transfer learning models, with some improvements","metadata":{}},{"cell_type":"markdown","source":"## First model\nTransfer learning: VGG16","metadata":{}},{"cell_type":"code","source":"# Constants & parameters\nVGG16_SIZE = 224\nbase_learning_rate = 0.01\noptimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\nloss = tf.keras.losses.CategoricalCrossentropy()\ninitial_epochs = 15\nBATCH_SIZE = 32\ninput_shape = (VGG16_SIZE, VGG16_SIZE, 3)\ndata_augmentation=data_augmenter()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Preprocessing of dataset\nmodel 01","metadata":{}},{"cell_type":"code","source":"# pre_process data\ndef preprocess_data01(\n    train_dataset, \n    validation_dataset, \n    test_dataset,\n    batch_size=BATCH_SIZE\n):\n\n    # rebatch\n    train_ds = train_dataset.rebatch(batch_size)\n    val_ds = validation_dataset.rebatch(batch_size)\n    test_ds = test_dataset.rebatch(batch_size)\n\n    # categorise\n    train_ds = train_ds.map(lambda x,y: (x, to_categorical(y, NUM_CLASSES)) )\n    val_ds = val_ds.map(lambda x,y: (x, to_categorical(y, NUM_CLASSES)) )\n    test_ds = test_ds.map(lambda x,y: (x, to_categorical(y, NUM_CLASSES)) )\n\n    # resize\n    resizing = Resizing(VGG16_SIZE, VGG16_SIZE)\n    train_ds = train_ds.map(lambda x,y: (resizing(x), y))\n    val_ds = val_ds.map(lambda x,y: (resizing(x), y))\n    test_ds = test_ds.map(lambda x,y: (resizing(x), y))\n\n    #rescale\n    normalization = Rescaling(1.0/255)\n    train_ds = train_ds.map(lambda x,y: (normalization(x), y))\n    val_ds = val_ds.map(lambda x,y: (normalization(x), y))\n\n    # prefetch\n    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n    val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n    test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return train_ds, val_ds, test_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds, val_ds, test_ds = preprocess_data01(train_dataset, validation_dataset, test_dataset, batch_size=BATCH_SIZE)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model creation","metadata":{}},{"cell_type":"code","source":"#Load the base pretrained VGG16 model. \ndef first_model(input_shape, data_augmentation):\n    base_model = vgg16.VGG16(\n                    input_shape=(VGG16_SIZE, VGG16_SIZE, 3),\n                    include_top=False,\n                    weights='imagenet'\n    )\n    base_model.trainable = False\n\n    inputs = Input(shape=input_shape)\n    x = data_augmentation(inputs) \n    x = vgg16.preprocess_input(x) \n    x = base_model(x, training=False)\n    x = Flatten()(x)  \n    x = Dense(8, activation='relu', name = 'FC01_32_relu')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(len(CLASS_NAMES), activation=\"softmax\")(x)\n\n    model = Model(inputs, x, name=\"1st_model\")\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model01 = first_model(input_shape, data_augmentation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model01.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model compilation","metadata":{}},{"cell_type":"code","source":"model01.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model train","metadata":{}},{"cell_type":"code","source":"history01 = model01.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=initial_epochs,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model evaluation","metadata":{}},{"cell_type":"code","source":"cmatrix01, report01 = evaluate(model01, test_ds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_evaluation(history01, cmatrix01, report01)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Second Model\nTransfer learning: MobileNetV2","metadata":{}},{"cell_type":"code","source":"# Constants & parameters\nbase_learning_rate = 0.01\noptimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\nloss = tf.keras.losses.CategoricalCrossentropy()\ninitial_epochs = 15\ninput_shape = IMG_SIZE + (3,)\nBATCH_SIZE = 32\ndata_augmentation=data_augmenter()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pre_process data\n# pre_process data\ndef preprocess_data02(\n    train_dataset, \n    validation_dataset, \n    test_dataset,\n    batch_size=BATCH_SIZE\n):\n\n    # rebatch\n    train_ds = train_dataset.rebatch(batch_size)\n    val_ds = validation_dataset.rebatch(batch_size)\n    test_ds = test_dataset.rebatch(batch_size)\n\n    # categorise\n    train_ds = train_ds.map(lambda x,y: (x, to_categorical(y, num_classes=NUM_CLASSES)) )\n    val_ds = val_ds.map(lambda x,y: (x, to_categorical(y, num_classes=NUM_CLASSES)) )\n    test_ds = test_ds.map(lambda x,y: (x, to_categorical(y, num_classes=NUM_CLASSES)) )\n\n    #rescale\n    normalization = Rescaling(1.0/255)\n    train_ds = train_ds.map(lambda x,y: (normalization(x), y))\n    val_ds = val_ds.map(lambda x,y: (normalization(x), y))\n\n    # prefetch\n    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n    val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n    test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return train_ds, val_ds, test_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds, val_ds, test_ds = preprocess_data02(train_dataset, validation_dataset, test_dataset, batch_size=BATCH_SIZE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def second_model(input_shape, data_augmentation):\n    base_model = mobilenet_v2.MobileNetV2(\n        input_shape=input_shape,\n        include_top=False,\n        weights='imagenet'\n    )\n    base_model.trainable = False\n    \n    inputs = Input(shape=input_shape)\n    x = data_augmentation(inputs)\n    x = mobilenet_v2.preprocess_input(x) \n    x = base_model(x, training=False)\n    x = GlobalMaxPooling2D()(x)\n    x = Dropout(0.2)(x)\n    prediction_layer = Dense(len(CLASS_NAMES), activation=\"softmax\")\n    outputs = prediction_layer(x)\n    model = Model(inputs, outputs, name=\"2nd_model\")\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model02 = second_model(input_shape, data_augmentation)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model02.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model02.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history02 = model02.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=initial_epochs,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmatrix02, report02 = evaluate(model02, test_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_evaluation(history02, cmatrix02, report02)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 03\nTransfer learning: ResNet50","metadata":{}},{"cell_type":"code","source":"# Constants & parameters\nbase_learning_rate = 0.01\noptimizer = tf.keras.optimizers.Adam(learning_rate=base_learning_rate)\nloss = tf.keras.losses.CategoricalCrossentropy()\ninitial_epochs = 15\ninput_shape = IMG_SIZE + (3,)\nBATCH_SIZE = 32\ndata_augmentation=data_augmenter()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pre_process data\ndef preprocess_data03(\n    train_dataset, \n    validation_dataset, \n    test_dataset,\n    batch_size=BATCH_SIZE\n):\n\n    # rebatch\n    train_ds = train_dataset.rebatch(batch_size)\n    val_ds = validation_dataset.rebatch(batch_size)\n    test_ds = test_dataset.rebatch(batch_size)\n\n    # categorise\n    train_ds = train_ds.map(lambda x,y: (x, to_categorical(y, num_classes=NUM_CLASSES)) )\n    val_ds = val_ds.map(lambda x,y: (x, to_categorical(y, num_classes=NUM_CLASSES)) )\n    test_ds = test_ds.map(lambda x,y: (x, to_categorical(y, num_classes=NUM_CLASSES)) )\n\n    #rescale\n    normalization = Rescaling(1.0/255)\n    train_ds = train_ds.map(lambda x,y: (normalization(x), y))\n    val_ds = val_ds.map(lambda x,y: (normalization(x), y))\n\n    # prefetch\n    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n    val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n    test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return train_ds, val_ds, test_ds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds, val_ds, test_ds = preprocess_data03(train_dataset, validation_dataset, test_dataset, batch_size=BATCH_SIZE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def third_model(input_shape, data_augmentation):\n    base_model = resnet50.ResNet50(\n        input_shape=input_shape,\n        include_top=False,\n        weights='imagenet'\n    )\n    base_model.trainable = False\n    \n    inputs = Input(shape=input_shape)\n    x = data_augmentation(inputs)\n    x = resnet50.preprocess_input(x) \n    x = base_model(x, training=False)\n    x = GlobalMaxPooling2D()(x)\n    x = Dropout(0.2)(x)\n    prediction_layer = Dense(len(CLASS_NAMES), activation=\"softmax\")\n    outputs = prediction_layer(x)\n    model = Model(inputs, outputs, name=\"3rd_model\")\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model03 = third_model(input_shape, data_augmentation)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model03.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model03.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history03 = model03.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=initial_epochs,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmatrix03, report03 = evaluate(model03, test_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_evaluation(history03, cmatrix03, report03)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusions\nMost models overfit the dataset, and it is not considered appropriated. Considering similar approaches from different sources, we will try to train with similar architecture from the previous ones, but with training parameters.","metadata":{}},{"cell_type":"markdown","source":"## Model 04\nConvolutional architecture: MobileNet","metadata":{}},{"cell_type":"code","source":"# Constants & parameters\noptimizer = tf.keras.optimizers.Adam()\nloss = tf.keras.losses.CategoricalCrossentropy()\ninitial_epochs = 20\ninput_shape = IMG_SIZE + (3,)\nBATCH_SIZE = 32\ndata_augmentation=data_augmenter()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pre_process data\ndef preprocess_data04(\n    train_dataset, \n    validation_dataset, \n    test_dataset,\n    batch_size=BATCH_SIZE\n):\n\n    # rebatch\n    train_ds = train_dataset.rebatch(batch_size)\n    val_ds = validation_dataset.rebatch(batch_size)\n    test_ds = test_dataset.rebatch(batch_size)\n\n    # categorise\n    train_ds = train_ds.map(lambda x,y: (x, to_categorical(y, num_classes=NUM_CLASSES)) )\n    val_ds = val_ds.map(lambda x,y: (x, to_categorical(y, num_classes=NUM_CLASSES)) )\n    test_ds = test_ds.map(lambda x,y: (x, to_categorical(y, num_classes=NUM_CLASSES)) )\n\n    #rescale\n    normalization = Rescaling(1.0/255)\n    train_ds = train_ds.map(lambda x,y: (normalization(x), y))\n    val_ds = val_ds.map(lambda x,y: (normalization(x), y))\n\n    # prefetch\n    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n    val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n    test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return train_ds, val_ds, test_ds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds, val_ds, test_ds = preprocess_data04(train_dataset, validation_dataset, test_dataset, batch_size=BATCH_SIZE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fourth_model(input_shape, data_augmentation):\n    base_model = mobilenet_v2.MobileNetV2(\n        input_shape=input_shape,\n        include_top=False,\n        weights='imagenet',\n        classifier_activation='softmax',\n    )\n    base_model.trainable = True\n    \n    inputs = Input(shape=input_shape)\n    x = data_augmentation(inputs)\n    x = mobilenet_v2.preprocess_input(x) \n    x = base_model(x, training=True)\n    x = GlobalAveragePooling2D()(x)\n    x = layers.Dense(128)(x) \n    x = layers.Activation(relu)(x) \n    x = layers.Dense(64)(x)\n    x = layers.Activation(relu)(x)\n    outputs = Dense(len(CLASS_NAMES), activation=softmax)(x)\n    model = Model(inputs, outputs, name=\"4th_model\")\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model04 = fourth_model(input_shape, data_augmentation)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model04.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model04.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmatrix04, report04 = evaluate(model04, test_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_evaluation(history04, cmatrix04, report04)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 5\nModel: ResNet50V2","metadata":{}},{"cell_type":"code","source":"# Constants & parameters\nbase_learning_rate = 0.01\noptimizer = tf.keras.optimizers.Adam()\nloss = tf.keras.losses.CategoricalCrossentropy()\ninitial_epochs = 20\ninput_shape = IMG_SIZE + (3,)\nBATCH_SIZE = 32\ndata_augmentation=data_augmenter()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pre_process data\ndef preprocess_data05(\n    train_dataset, \n    validation_dataset, \n    test_dataset,\n    batch_size\n):\n\n    # rebatch\n    train_ds = train_dataset.rebatch(batch_size)\n    val_ds = validation_dataset.rebatch(batch_size)\n    test_ds = test_dataset.rebatch(batch_size)\n\n    # categorise\n    train_ds = train_ds.map(lambda x,y: (x, to_categorical(y, num_classes=NUM_CLASSES)) )\n    val_ds = val_ds.map(lambda x,y: (x, to_categorical(y, num_classes=NUM_CLASSES)) )\n    test_ds = test_ds.map(lambda x,y: (x, to_categorical(y, num_classes=NUM_CLASSES)) )\n\n    #rescale\n    normalization = Rescaling(1.0/255)\n    train_ds = train_ds.map(lambda x,y: (normalization(x), y))\n    val_ds = val_ds.map(lambda x,y: (normalization(x), y))\n\n    # prefetch\n    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n    val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n    test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return train_ds, val_ds, test_ds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds, val_ds, test_ds = preprocess_data05(train_dataset, validation_dataset, test_dataset, batch_size=BATCH_SIZE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fifth_model(input_shape, data_augmentation):\n    base_model = resnet_v2.ResNet50V2(\n        input_shape=input_shape,\n        include_top=False,\n        weights='imagenet'\n    )\n    base_model.trainable = True\n    \n    inputs = Input(shape=input_shape)\n    x = data_augmentation(inputs)\n    x = resnet_v2.preprocess_input(x) \n    x = base_model(x, training=True)\n    x = GlobalAveragePooling2D()(x)\n    x = layers.Dense(128)(x) \n    x = layers.Activation(relu)(x) \n    x = layers.Dense(64)(x)\n    x = layers.Activation(relu)(x)\n    outputs = Dense(len(CLASS_NAMES), activation=\"softmax\")(x)\n    model = Model(inputs, outputs, name=\"5th_model\")\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model05 = fifth_model(input_shape, data_augmentation)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model05.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model05.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history05 = model05.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=initial_epochs,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmatrix05, report05 = evaluate(model05, test_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_evaluation(history05, cmatrix05, report05)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 06","metadata":{}},{"cell_type":"code","source":"# Constants & parameters\noptimizer = tf.keras.optimizers.Adam()\nloss = \"sparse_categorical_crossentropy\"\ninitial_epochs = 20\ninput_shape = IMG_SIZE + (3,)\nBATCH_SIZE = 32\ndata_augmentation=data_augmenter()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pre_process data\ndef preprocess_data06(\n    train_dataset, \n    validation_dataset, \n    test_dataset,\n    batch_size\n):\n\n    # rebatch\n    train_ds = train_dataset.rebatch(batch_size)\n    val_ds = validation_dataset.rebatch(batch_size)\n    test_ds = test_dataset.rebatch(batch_size)\n    \n    # reshape\n    \n\n    #rescale\n    normalization = Rescaling(1.0/255)\n    train_ds = train_ds.map(lambda x,y: (normalization(x), y))\n    val_ds = val_ds.map(lambda x,y: (normalization(x), y))\n\n    # prefetch\n    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n    val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n    test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return train_ds, val_ds, test_ds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds, val_ds, test_ds = preprocess_data06(train_dataset, validation_dataset, test_dataset, batch_size=BATCH_SIZE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sixth_model(input_shape, data_augmentation):\n    base_model = resnet_v2.ResNet50V2(\n        input_shape=input_shape,\n        include_top=False,\n        weights='imagenet'\n    )\n    base_model.trainable = True\n    \n    inputs = Input(shape=input_shape)\n    x = data_augmentation(inputs)\n    x = resnet_v2.preprocess_input(x) \n    x = base_model(x, training=True)\n    x = GlobalAveragePooling2D()(x)\n    x = layers.Dense(128)(x) \n    x = layers.Activation(relu)(x) \n    x = layers.Dense(64)(x)\n    x = layers.Activation(relu)(x)\n    outputs = Dense(len(CLASS_NAMES), activation=\"softmax\")(x)\n    model = Model(inputs, outputs, name=\"6th_model\")\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model06 = sixth_model(input_shape, data_augmentation)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model06.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model06.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history06 = model06.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=initial_epochs,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmatrix06, report06 = evaluate(model06, test_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_evaluation(history06, cmatrix06, report06)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model 07","metadata":{}},{"cell_type":"code","source":"# Constants & parameters\noptimizer = tf.keras.optimizers.Adam()\nloss = \"sparse_categorical_crossentropy\"\ninitial_epochs = 20\ninput_shape = IMG_SIZE + (3,)\nBATCH_SIZE = 32\ndata_augmentation=data_augmenter()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pre_process data\ndef preprocess_data07(\n    train_dataset, \n    validation_dataset, \n    test_dataset,\n    batch_size=BATCH_SIZE\n):\n\n    # rebatch\n    train_ds = train_dataset.rebatch(batch_size)\n    val_ds = validation_dataset.rebatch(batch_size)\n    test_ds = test_dataset.rebatch(batch_size)\n\n    #rescale\n    normalization = Rescaling(1.0/255)\n    train_ds = train_ds.map(lambda x,y: (normalization(x), y))\n    val_ds = val_ds.map(lambda x,y: (normalization(x), y))\n\n    # prefetch\n    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n    val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n    test_ds = test_ds.prefetch(buffer_size=AUTOTUNE)\n    \n    return train_ds, val_ds, test_ds","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds, val_ds, test_ds = preprocess_data07(train_dataset, validation_dataset, test_dataset, batch_size=BATCH_SIZE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seventh_model(input_shape, data_augmentation):\n    base_model = mobilenet_v2.MobileNetV2(\n        input_shape=input_shape,\n        include_top=False,\n        weights='imagenet',\n        classifier_activation='softmax',\n    )\n    base_model.trainable = True\n    \n    inputs = Input(shape=input_shape)\n    x = data_augmentation(inputs)\n    x = mobilenet_v2.preprocess_input(x) \n    x = base_model(x, training=True)\n    x = GlobalAveragePooling2D()(x)\n    x = layers.Dense(128)(x) \n    x = layers.Activation(relu)(x) \n    x = layers.Dense(64)(x)\n    x = layers.Activation(relu)(x)\n    outputs = Dense(len(CLASS_NAMES), activation=softmax)(x)\n    model = Model(inputs, outputs, name=\"7th_model\")\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model07 = seventh_model(input_shape, data_augmentation)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model07.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model07.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history07 = model07.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=initial_epochs,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cmatrix07, report07 = evaluate(model07, test_ds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_evaluation(history07, cmatrix07, report07)","metadata":{},"execution_count":null,"outputs":[]}]}