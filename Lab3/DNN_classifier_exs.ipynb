{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAB: DNN with keras and tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1: MNIST dataset (Digit recognition dataset)**\n",
    "\n",
    "MNIST is available as a keras dataset (mnist)\n",
    "\n",
    "Inputs: images of 28 x 28 pixels\n",
    "\n",
    "Output: class representing the digit (10 classes, digitos 0-9)\n",
    "\n",
    "60k images for training and 10k images for test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels)= mnist.load_data()\n",
    "\n",
    "#Check data dimensions\n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "\n",
    "**Classification problem => Convert outputs into categorical variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Reshape to make the images into 1D vector  for each example\n",
    "train_images = ?\n",
    "\n",
    "#Standardize values /255\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "#Convert outputs into categorical variables \n",
    "train_labels = to_categorical(train_labels)\n",
    "\n",
    "#Do the same for the test data\n",
    " ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model structure (feedforward DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "#One hidden layer with 512 neurons (RelU)\n",
    "network.add(layers.Dense(512, activation='relu', \n",
    "                         input_shape=(28 * 28,)))\n",
    "\n",
    "# Output layer with 10 neurons\n",
    "# (softmax = 1 neuron for each class; one-hot encoding )\n",
    "network.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "#train the model (fit)\n",
    "history= network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***history.history* is a dictionary, check what are the keys, extract their values and plot them. You are expected to get similar plots as below.** \n",
    "\n",
    "<img src=\"images/f1.jpg\" style=\"width:250px;height:150px;\">\n",
    "\n",
    "<img src=\"images/f2.jpg\" style=\"width:250px;height:150px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predict the outputs (with *network.predict*)  for test images with the trained DNN network**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = ?\n",
    "\n",
    "# test_preds contains the class probabilities, what is its dimension ?\n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(choose the class with max probabilities)\n",
    "test_classes = np.argmax(network.predict(test_images), axis=-1)\n",
    "\n",
    "#what is the dimension of test_classes ?\n",
    " ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the trained model for test and train images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels, verbose =0)\n",
    "print(test_loss, test_acc)\n",
    "\n",
    "#Do the same for train data. Compare the results \n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:  DNN classifier for IMDB dataset\n",
    "\n",
    "Dataset with texts of reviews in IMDB about movies; classified into 2 \n",
    "classes: positive or negative\n",
    "\n",
    "25k reviews for training + 25k for test; \n",
    "\n",
    "balanced – 50% positive and negative examples\n",
    "\n",
    "IMDB is available as a keras dataset (imdb)\n",
    "\n",
    "We will only consider the most common 10k words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "#Use text dictionary with num_words=10000\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dimensions of train and test data, labels\n",
    "?\n",
    "\n",
    "#what is the content of train_data and train_labels ? \n",
    "?\n",
    "\n",
    "#Print the content of the first review \n",
    "?\n",
    "#The number of words in each review are different \n",
    "# How many words are in review 1, review 4\n",
    "\n",
    "? # ANSWER: 218 words / #550 words \n",
    "\n",
    "#What is the class of the first review\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transform the word indices back into words** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max([max(sequence) for sequence in train_data]))\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[2]])\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "#One-hot encoding of the reviews (binary vectors)\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "#Output to binary (0 / 1 Classes) \n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model structure (fully connected feedforward DNN)\n",
    "\n",
    "Implements DNN  with 2 hidden layers; \n",
    "\n",
    "16 neurons in each hidden layers; RelU\n",
    "\n",
    "Output layer – sigmoid – binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "\n",
    "?\n",
    "?\n",
    "?\n",
    "model = ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN Model training  for IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with optimizer (RMSprop), \n",
    "# loss function (binary_crossentropy ) and error metric (accuracy)\n",
    "\n",
    "?\n",
    "\n",
    "#Split train data into: \n",
    "# validation data (the first 10000 reviews in x_train)\n",
    "# train data (the remaining reviews in x_train )\n",
    "\n",
    "x_val = \n",
    "y_val = \n",
    "\n",
    "partial_x_train = \n",
    "partial_y_train = \n",
    "\n",
    "#Fit the model with partial train data and validation data \n",
    "#(check model.fit function of keras)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot results similar to the figures below. \n",
    "\n",
    "<img src=\"images/f3.jpg\" style=\"width:250px;height:150px;\">\n",
    "\n",
    "<img src=\"images/f4.jpg\" style=\"width:250px;height:150px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test data \n",
    " ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on train data. \n",
    "# what are the loss and the accuracy ?\n",
    "\n",
    "?\n",
    "\n",
    "# Do the same for test data and compare\n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting issues - change the number of hidden layer units\n",
    "\n",
    "If you observe a significant difference (7%, 8%) between train and test accuracy, try different approaches. \n",
    "\n",
    "Change the number of hidden layer units (h). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, repeat all training and testing process with h = 4 or 8. \n",
    "hidden = 4  \n",
    "\n",
    "#build the model\n",
    "?\n",
    "#fit the model\n",
    "?\n",
    "#plot train and val acc, train and val loss during the training\n",
    "?\n",
    "# evaluate the model, what are the train and test final accuracies \n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting issues - Regularization  L2\n",
    "\n",
    "Apply L2, but it can be changed to L1 or L1+ L2: for example\n",
    "\n",
    "regularizers.l1(0.001)\n",
    "\n",
    "regularizers.l1_l2(l1=0.001, l2=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "hidden = 16\n",
    "\n",
    "model_l2 = models.Sequential()\n",
    "\n",
    "model_l2.add(layers.Dense(hidden, activation='relu', \n",
    "  kernel_regularizer=regularizers.l2(0.001),input_shape=(10000,)))\n",
    "\n",
    "model_l2.add(layers.Dense(hidden, activation='relu', \n",
    "  kernel_regularizer=regularizers.l2(0.001)))\n",
    "\n",
    "model_l2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "#Compile and fit the model \n",
    "?\n",
    "#plot train and val acc, train and val loss during the training\n",
    "?\n",
    "# evaluate the model, what are the train and test final accuracies \n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting issues - Regularization  L1\n",
    "\n",
    "Apply now Regularization  L1 for the same model and repeat the whole training and testing process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting issues - dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dr = models.Sequential()\n",
    "model_dr.add(layers.Dense(hidden, activation='relu', \n",
    "                          input_shape=(10000,)))\n",
    "model_dr.add(layers.Dropout(0.5))\n",
    "\n",
    "model_dr.add(layers.Dense(hidden, activation='relu'))\n",
    "model_dr.add(layers.Dropout(0.5))\n",
    "\n",
    "model_dr.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#Compile and fit the model \n",
    "?\n",
    "#plot train and val acc, train and val loss during the training\n",
    "?\n",
    "# evaluate the model, what are the train and test final accuracies \n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting issues -  EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "hidden = 16\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(hidden, activation='relu', \n",
    "                       input_shape=(10000,)))\n",
    "model.add(layers.Dense(hidden, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "early = EarlyStopping(monitor='val_loss', min_delta=0, patience= 5, \n",
    "                      verbose= True, mode='auto')\n",
    "\n",
    "callbacks = [early]\n",
    "history_es = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val),\n",
    "                    callbacks = callbacks)\n",
    "\n",
    "#plot train and val acc, train and val loss during the training\n",
    "?\n",
    "# evaluate the model, what are the train and test final accuracies \n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting issues -  Dropout & EarlyStopping\n",
    "\n",
    "Aplly a combination of mechanisms to deal with overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
