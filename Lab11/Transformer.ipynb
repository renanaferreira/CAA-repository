{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Transformer Assignment - Subclass.ipynb","provenance":[]},"coursera":{"schema_names":["DLSC5W4-1A"]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"widgets":{"application/vnd.jupyter.widget-state+json":{"08b8eb7e54b0420692f439151110ea78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a104536dc844996864bb00e07c351ff","placeholder":"​","style":"IPY_MODEL_77e7f5504f1a482980058bdd07d07d5e","value":" 220/220 [14:41&lt;00:00,  4.01s/it]"}},"0a7b94269612418ab913737ada1a606f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f138da56eae488a85970020606e093d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7dd6278751c440baf1b80c19cdac901","placeholder":"​","style":"IPY_MODEL_32f739f4c4e742bc837232c8616c85c3","value":" 442/442 [00:00&lt;00:00, 1.40kB/s]"}},"1171e92377e74bbd855dcf2cd49cd0fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"156f653b1cf74c88a77e5ea66d59dddd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"32f739f4c4e742bc837232c8616c85c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a104536dc844996864bb00e07c351ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47460b36e9524825ab8228e5efae338e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bf260e4350dd4037bce3badc2c076b94","IPY_MODEL_08b8eb7e54b0420692f439151110ea78"],"layout":"IPY_MODEL_156f653b1cf74c88a77e5ea66d59dddd"}},"4ed12cd5a3a943ae862f026e20d0e9e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"52dbd62baaf04233a14a4278c9f5d24f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77e7f5504f1a482980058bdd07d07d5e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8427a9c9ac78457cbdfffd41d76a467d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1171e92377e74bbd855dcf2cd49cd0fc","placeholder":"​","style":"IPY_MODEL_b4c3e99db1374b1b83b768dc1dc7c470","value":" 363M/363M [00:06&lt;00:00, 56.4MB/s]"}},"84748d3f92494e839763a1ed7e3dc45e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a411c46cc2194664a08e1bc2eba903ff","IPY_MODEL_0f138da56eae488a85970020606e093d"],"layout":"IPY_MODEL_8b266584bf764912abb999040e1c4643"}},"8b266584bf764912abb999040e1c4643":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b8eb55aa69341148bc0fd72a5cc383d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"a411c46cc2194664a08e1bc2eba903ff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_ae662a275a7f443c868463b275454359","max":442,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a9d8813c381c43d5a17594a067f0c8c9","value":442}},"a9d8813c381c43d5a17594a067f0c8c9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"ae662a275a7f443c868463b275454359":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4c3e99db1374b1b83b768dc1dc7c470":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf260e4350dd4037bce3badc2c076b94":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"100%","description_tooltip":null,"layout":"IPY_MODEL_0a7b94269612418ab913737ada1a606f","max":220,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9b8eb55aa69341148bc0fd72a5cc383d","value":220}},"c13de54036014187a3c9e532e15112be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2a1ddc5ec3d4e828d4abe111d0e3957":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_52dbd62baaf04233a14a4278c9f5d24f","max":363423424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4ed12cd5a3a943ae862f026e20d0e9e5","value":363423424}},"ded975cd21d640d7847ecaadd45387b3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c2a1ddc5ec3d4e828d4abe111d0e3957","IPY_MODEL_8427a9c9ac78457cbdfffd41d76a467d"],"layout":"IPY_MODEL_c13de54036014187a3c9e532e15112be"}},"f7dd6278751c440baf1b80c19cdac901":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Lab: Transformer Network\n\nIn this notebook you'll explore the Transformer architecture, a neural network that takes advantage of parallel processing and allows to speed up the training process. \n\n**After this assignment you'll be able to**:\n\n* Create positional encodings to capture sequential relationships in data\n* Calculate scaled dot-product self-attention with word embeddings\n* Implement masked multi-head attention\n* Build a Transformer model","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AbzZLqIPv6b7","outputId":"19f2fc2b-6f1d-4b43-fd50-4c513e3936fd"}},{"cell_type":"code","source":"#pip install transformers !!!\n\nimport tensorflow as tf\nimport pandas as pd\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\nfrom transformers import DistilBertTokenizerFast #, TFDistilBertModel\nfrom transformers import TFDistilBertForTokenClassification\nfrom tqdm import tqdm_notebook as tqdm","metadata":{"id":"_OpwqWL2QH5G"},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### 1 - Positional Encoding\n\nIn sequence to sequence tasks, the relative order of  data is important to its meaning. When training sequential neural networks such as RNNs, you fed the inputs into the network in order. Information about the order of the data was automatically fed into the model.  However, when you train a Transformer network, you feed the data into the model all at once. While this reduces training time, there is no information about the order of the data. This is where positional encoding is useful - you can specifically encode the positions of the inputs and pass them into the network using these sine and cosine formulas:\n    \n$$\nPE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n\\tag{1}$$\n<br>\n$$\nPE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n\\tag{2}$$\n\n* $d$ is the dimension of the word embedding and positional encoding\n* $pos$ is the position of the word.\n* $i$ refers to each of the different dimensions of the positional encoding.\n\nThe values of the sine and cosine equations are small enough (between -1 and 1) that when you add the positional encoding to a word embedding, the word embedding is not significantly distorted. The sum of the positional encoding and word embeding is ultimately what is fed into the model. Using a combination of these two equations helps Transformer network attend to the relative positions of the input data. In this assignment, all vectors are horizontal. All matrix multiplications are adjusted accordingly.\n\n### 1.1 - Get Sine and Cosine Angles\n\nGet the possible angles used to compute the positional encodings by calculating the inner term of the sine and cosine equations: \n\n$$\\frac{pos}{10000^{\\frac{2i}{d}}} \\tag{3}$$\n\nComplete `get_angles()` to get the possible angles for sine and cosine positional encodings.","metadata":{}},{"cell_type":"code","source":"def get_angles(pos, i, d):\n    \"\"\"\n    Get the angles for the positional encoding\n    \nArguments:\npos -- Column vector containing the positions [[0], [1], ...,[N-1]]\ni --   Row vector containing the dimension span [[0, 1, 2, ..., M-1]]\nd(integer) -- Encoding/embedding size\n    \n    Returns:\n        angles -- (pos, d) numpy array \n    \"\"\"\n    angles = ?\n    \n    return angles","metadata":{"id":"bPzwMVfcQpT-"},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### 1.2 - Sine and Cosine Positional Encodings\n\nThe computed angles are used to calculate the sine and cosine positional encodings.\n\n$$\nPE_{(pos, 2i)}= sin\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n$$\n<br>\n$$\nPE_{(pos, 2i+1)}= cos\\left(\\frac{pos}{{10000}^{\\frac{2i}{d}}}\\right)\n$$\n\nFunction `positional_encoding()` calculates sine and cosine  positional encodings\n\n**NOTE:** Sine equation is used when $i$ is even number and cosine equation when $i$ is odd number.","metadata":{}},{"cell_type":"code","source":"def positional_encoding(positions, d):\n    \"\"\"\n    Precomputes a matrix with all the positional encodings \n    \n    Arguments:\n        positions (int) -- Maximum number of positions to be encoded \n        d (int) -- Encoding/embedding size \n    \n    Returns:\npos_encoding: matrix with the positional encodings, shape (1,position,d) \n    \"\"\"\n    # initialize a matrix angle_rads of all the angles \n# Transform from row to column vector (np.newaxis add new dimension)\n    angle_rads = get_angles(np.arange(positions)[:, np.newaxis],\n                            np.arange(d)[ np.newaxis,:],d)\n  \n    # -> angle_rads has dim (positions,d)\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n  \n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n     \n    pos_encoding = angle_rads[np.newaxis, ...]\n    \n    #tf.cast change the type of tensor to float32\n    return tf.cast(pos_encoding, dtype=tf.float32)","metadata":{"id":"y78txxoHQtwG"},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#Test function positional_encoding for some small values of the arguments\n\n?","metadata":{},"execution_count":18,"outputs":[{"name":"stdout","output_type":"stream","text":"(1, 3, 5)\n"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 3, 5), dtype=float32, numpy=\n","array([[[0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n","         0.0000000e+00],\n","        [8.4147096e-01, 9.9968451e-01, 6.3095731e-04, 1.0000000e+00,\n","         3.9810718e-07],\n","        [9.0929741e-01, 9.9873835e-01, 1.2619144e-03, 1.0000000e+00,\n","         7.9621435e-07]]], dtype=float32)>"]},"metadata":{}}]},{"cell_type":"markdown","source":"### 2 - Masking\n\nThere are two types of masks when building Transformer network: *padding mask* and *look-ahead mask*. Both help the softmax computation give the appropriate weights to the words in the input sentence. \n\n### 2.1 - Padding Mask\n\nIf the input sequence exceeds the maximum length of a sequence the network can process. Let's say the maximum length of the model is 5, it is fed the following sequences:\n\n    [[\"Do\",\"you\",\"know\",\"when\",\"Jane\",\"is\",\"going\",\"to\", \"visit\", Africa\"], \n     [\"Jane\", \"visits\", \"Africa\", \"in\", \"September\" ],\n     [\"Exciting\", \"!\"]\n    ]\n\nwhich is vectorized as:\n\n    [[ 71, 121, 4, 56, 99, 2344, 345, 1284, 15],\n     [ 56, 1285, 15, 181, 545],\n     [ 87, 600]\n    ]\n    \nWhen passing sequences into a transformer model, it is important that they are of uniform length. You can achieve this by padding the sequence with zeros, and truncating sentences that exceed the maximum length of the model:\n\n    [[ 71, 121, 4, 56, 99],\n     [ 2344, 345, 1284, 15, 0],\n     [ 56, 1285, 15, 181, 545],\n     [ 87, 600, 0, 0, 0],\n    ]\n    \nSequences longer than the maximum length of 5 will be truncated, and zeros will be added to the truncated sequence to achieve uniform length. Similarly, for sequences shorter than the maximum length, zeros will be added for padding. However, these zeros will affect the softmax calculation - this is when a padding mask comes in handy! By multiplying a padding mask by -1e9 and adding it to the sequence, you mask out the zeros by setting them close to negative infinity. \n\nAfter masking, the input should go from `[87, 600, 0, 0, 0]` to `[87, 600, -1e9, -1e9, -1e9]`, so that when you take the softmax, the zeros don't affect the score.","metadata":{}},{"cell_type":"code","source":"def create_padding_mask(seq):\n    \"\"\"\n    Creates a matrix mask for the padding cells\n    \n    Arguments:\n        seq -- (n, m) matrix\n    \n    Returns:\n        mask -- (n, 1, 1, m) binary tensor\n    \"\"\"\n#tf.math.equal(x, 0) = give back binary tensor, \n# if x==0 is true give back 1 if it is not true give back 0\n\n#tf.cast change the type of tensor to float32\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32) #(n,m)\n  \n    # add extra dimensions to add the padding to the attention logits.\n    return seq[:, tf.newaxis, tf.newaxis, :] #(n, 1, 1, m)","metadata":{"id":"JOL9XWsFQxxo"},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"x = tf.constant([[7., 6., 0., 0., 1.], [1., 2., 3., 0., 0.], [0., 0., 0., 4., 5.]])\n\n#Apply mask to x and check what is the result ?\n\n?","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5J5FFjklQ1Fz","outputId":"8319446f-3ed4-406a-cf38-ca2b08142ff4","scrolled":true},"execution_count":6,"outputs":[{"name":"stdout","output_type":"stream","text":"tf.Tensor(\n\n[[7. 6. 0. 0. 1.]\n\n [1. 2. 3. 0. 0.]\n\n [0. 0. 0. 4. 5.]], shape=(3, 5), dtype=float32)\n\n\n\ntf.Tensor(\n\n[[[[0. 0. 1. 1. 0.]]]\n\n\n\n\n\n [[[0. 0. 0. 1. 1.]]]\n\n\n\n\n\n [[[1. 1. 1. 0. 0.]]]], shape=(3, 1, 1, 5), dtype=float32)\n"}]},{"cell_type":"markdown","source":"If we multiply this mask by -1e9 and add it to the sample input sequences, the zeros are essentially set to negative infinity.","metadata":{}},{"cell_type":"markdown","source":"### 2.2 - Look-ahead Mask\n\nThe look-ahead mask follows similar intuition. In training, you will have access to the complete correct output of the training example. The look-ahead mask helps your model pretend that it correctly predicted a part of the output and see if, *without looking ahead*, it can correctly predict the next output. \n\nFor example, if the expected correct output is `[1, 2, 3]` and you wanted to see if given that the model correctly predicted the first value it could predict the second value, you would mask out the second and third values. So you would input the masked sequence `[1, -1e9, -1e9]` and see if it could generate `[1, 2, -1e9]`.","metadata":{}},{"cell_type":"code","source":"def create_look_ahead_mask(size):\n    \"\"\"\n    Returns an lower triangular matrix filled with ones\n    \n    Arguments:\n        size -- matrix size\n    \n    Returns:\n        mask -- (size, size) tensor\n    \"\"\"\n    # Transform square (size, size) tensor into: \n    # lower triangular part =1\n    # Upper triangular part = 0\n    mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask ","metadata":{"id":"9O9UbM31Q3hK"},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Generate a row vector with 5 random values with uniform distribution \n#  in the range (0,1)\nx = ?\n\n#Apply Look-ahead Mask and observe the result\n\n?\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfzHoVj9Q5nG","outputId":"300e76ec-77d0-460a-b6df-71e40de86606","scrolled":true},"execution_count":23,"outputs":[{"name":"stdout","output_type":"stream","text":"[0.90884209 0.37910562 0.05240325 0.69914559 0.9440902 ]\n\n(5,)\n\n\n"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n","array([[1., 0., 0., 0., 0.],\n","       [1., 1., 0., 0., 0.],\n","       [1., 1., 1., 0., 0.],\n","       [1., 1., 1., 1., 0.],\n","       [1., 1., 1., 1., 1.]], dtype=float32)>"]},"metadata":{}}]},{"cell_type":"markdown","source":"### 3 - Self-Attention\n\nAs the authors of the Transformers paper state, \"Attention is All You Need\". \n\n<img src=\"images/self-attention.png\" alt=\"Encoder\" width=\"600\"/>\n<caption><center><font color='purple'><b>Figure 1: Self-Attention calculation visualization</font></center></caption>\n    \nThe use of self-attention paired with traditional networks allows for the parallization which speeds up training. Function **scaled dot product attention** takes in a query, key, value, and a mask as inputs to returns rich, attention-based vector representations of the words in a sequence. This type of self-attention can be mathematically expressed as:\n$$\n\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\\tag{4}\\\n$$\n\n* $Q$ is the matrix of queries \n* $K$ is the matrix of keys\n* $V$ is the matrix of values\n* $M$ is the optional mask to apply \n* ${d_k}$ is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode\n\n\nFunction **scaled_dot_product_attention()** creates attention-based representations. \n    \n**Note**: The boolean mask parameter can be passed as `none` or as padding or look-ahead. ","metadata":{"id":"VG0gPyv0oDBi"}},{"cell_type":"code","source":"def scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"\n    Calculate the attention weights.\n      q, k, v must have matching leading dimensions.\n    k, v must have matching penultimate dimension, \n    i.e.: seq_len_k = seq_len_v.\nMask has different shapes depending on the type (padding /look ahead)\n\n    Arguments:\n        q -- query shape == (..., seq_len_q, depth)\n        k -- key shape == (..., seq_len_k, depth)\n        v -- value shape == (..., seq_len_v, depth_v)\n        mask: Float tensor with shape broadcastable \n              to (..., seq_len_q, seq_len_k). Defaults to None.\n\n    Returns:\n        output -- attention_weights\n    \"\"\"  \n    # Q*K'\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n    # add mask to the scaled tensor\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)\n\n    # softmax is normalized on the last axis (seq_len_k) so that the\n    # scores add up to 1\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n    \n    # attention_weights * V\n    output = tf.matmul(attention_weights, v) # (..., seq_len_q, depth_v)\n\n    return output, attention_weights","metadata":{"id":"CSysk_rjQ7lp"},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### 4 - Encoder\n\nHere, the Transformer Encoder will be implemented by pairing multi-head attention and a Feed Forward Neural Network- FFNN (Figure 2a). \n<img src=\"images/encoder_layer.png\" alt=\"Encoder\" width=\"250\"/>\n<caption><center><font color='purple'><b>Figure 2a: Transformer encoder layer</font></center></caption>\n\n* `MultiHeadAttention (MHA)`  you can think as computing self-attention several times to detect different features. \n* FFNN contains two Dense layers implemented as `FullyConnected`\n\nInput sentence first passes through a *multi-head attention layer*, where the encoder looks at other words in the input sentence as it encodes a specific word. The outputs of the multi-head attention layer are then fed to a FFNN. The same FFNN is independently applied to each position.\n   \n* For the `MultiHeadAttention (MHA)` layer, you will use the [Keras implementation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention). If you're curious about how to split the query matrix Q, key matrix K, and value matrix V into different heads, you can look through the implementation. \n* You will also use the [Sequential API](https://keras.io/api/models/sequential/) with two dense layers to built the FFNN layers.","metadata":{"id":"blS0pEpTqRVI"}},{"cell_type":"code","source":"def FullyConnected(embedding_dim, fully_connected_dim):\n    return tf.keras.Sequential([\n        # (batch_size, seq_len, dff)\n        tf.keras.layers.Dense(fully_connected_dim, activation='relu'), \n        # (batch_size, seq_len, d_model)\n        tf.keras.layers.Dense(embedding_dim)  \n    ])","metadata":{"id":"sC5vJhz29vZR"},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### 4.1 Encoder Layer\n\nHere `EncoderLayer()` (Fig. 2a) is implemented using `call()` method. Residual connections and layer normalization are used to speed up training. The function performs the following steps: \n1. Pass Q, V, K matrices and a boolean mask to a multi-head attention layer. \n2. Pass the output of the multi-head attention layer to a dropout layer. The `training` parameter set the mode of the model. \n3. Add a skip connection by adding the original input `x` and the output of the dropout layer. \n4. Pass the output through the first layer normalization.\n5. Pass the output through the FFNN.  \n6. Add a dropout layer, add a skip connection, apply layer normalization. \n\n**Additional Hints**:\n* `__init__` method creates all the layers that will be accesed by the `call` method. Wherever you want to use a layer defined inside `__init__`  method you will have to use the syntax `self.[insert layer name]`. \n* You may find the documentation of [MultiHeadAttention](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MultiHeadAttention) helpful.","metadata":{"id":"R65WbX5wqYYH"}},{"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n    \"\"\"\nThe encoder layer is composed by a multi-head attention (MHA) mechanism,\nfollowed by a fully connected feed-forward network (FFNN). \nThis archirecture includes a residual connection around each of the two \nsub-layers, followed by layer normalization.\n    \"\"\"\n    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(num_heads=num_heads,\n                                      key_dim=embedding_dim)\n\n        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n                            fully_connected_dim=fully_connected_dim)\n\n        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n\n        self.dropout1 = Dropout(dropout_rate)\n        self.dropout2 = Dropout(dropout_rate)\n    \n    def call(self, x, training, mask):\n        \"\"\"\n        Forward pass for the Encoder Layer\n        \n        Arguments:\n        x -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n        training -- Boolean, set to true to activate\n                        the training mode for dropout layers\n        mask -- Boolean mask to ensure that the padding is not \n                    treated as part of the input\n    Returns:\n    out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n        \"\"\"\n          # calculate self-attention using MHA\n# To compute self-attention, Q, V and K are inicialized with x\n# Output shape: (batch_size, input_seq_len, embedding_dim)\n        self_attn_output = self.mha(x, x, x, mask) \n        \n        # apply dropout layer to the self-attention output\n        self_attn_output = self.dropout1(self_attn_output, training=training)\n        \n# apply layer normalization on sum of the input and the attention \n# output to get the output of MHA layer \n# Output shape: (batch_size, input_seq_len, embedding_dim)\n        mult_attn_out = self.layernorm1(x + self_attn_output)  \n\n# pass the output of  MHA layer through a ffn (FFNN) \n# Output shape: (batch_size, input_seq_len, embedding_dim)\n        ffn_output = self.ffn(mult_attn_out)  \n        \n        # apply dropout layer to ffn output \n        ffn_output = self.dropout2(ffn_output, training=training)\n        \n# apply normalization on sum of the output from MHA and ffn output\n# to get the output of the encoder layer \n        encoder_layer_out = self.layernorm2(ffn_output + mult_attn_out)  # (batch_size, input_seq_len, embedding_dim)\n        \n        return encoder_layer_out","metadata":{"id":"tIufbrc-9_2u"},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### 4.2 - Full Encoder\n\nNow we will build the full Transformer Encoder (Fig. 2b). \nFirst the input is embedded and positional encodings is added. Then feed the encoded embeddings to a stack of Encoder layers. \n\n<img src=\"images/encoder.png\" alt=\"Encoder\" width=\"330\"/>\n<caption><center><font color='purple'><b>Figure 2b: Transformer Encoder</font></center></caption>\n\n`Encoder()` function is implemented with `call()` method. The Encoder is initialized with an Embedding layer, positional encoding, and multiple EncoderLayers. The `call()` method performs the following steps: \n1. Pass the input through the Embedding layer.\n2. Scale the embedding by multiplying it by the square root of the embedding dimension. Cast the embedding dimension to data type `tf.float32` before computing the square root.\n3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to the embedding.\n4. Pass encoded embedding through dropout layer. Use`training` parameter to set the training mode. \n5. Pass the output of the dropout layer through the stack of encoding layers using a for loop.","metadata":{}},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    \"\"\"\nThe entire Encoder starts by passing the input to an embedding layer \nand using positional encoding to then pass the output through a stack \nof encoder Layers\n        \n    \"\"\"   \n    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n        maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(Encoder, self).__init__()\n\n        self.embedding_dim = embedding_dim\n        self.num_layers = num_layers\n\n        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, \n                                                self.embedding_dim)\n\n\n        self.enc_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n                            num_heads=num_heads,\n                            fully_connected_dim=fully_connected_dim,\n                            dropout_rate=dropout_rate,\n                            layernorm_eps=layernorm_eps) \n                           for _ in range(self.num_layers)]\n\n        self.dropout = Dropout(dropout_rate)\n        \n    def call(self, x, training, mask):\n        \"\"\"\n        Forward pass for the Encoder\n        \n        Arguments:\n            x -- Tensor of shape (batch_size, input_seq_len)\n            training -- Boolean, set to true to activate\n                        training mode for dropout layers\n            mask -- Boolean mask to ensure that the padding is not \n                    treated as part of the input\n        Returns:\n    out2 -- Tensor of shape (batch_size, input_seq_len, embedding_dim)\n        \"\"\"\n\n        seq_len = tf.shape(x)[1]\n        \n        # Pass input through Embedding layer\n        # Output shape: (batch_size, input_seq_len, embedding_dim)\n        x = self.embedding(x) \n        \n# Scale embedding multiplying by square root of the embedding dimension\n        x *= tf.math.sqrt(tf.cast(self.embedding_dim,tf.float32))\n        \n        # Add position encoding to embedding\n        x += self.pos_encoding[:, :seq_len, :]\n        \n        # Pass the encoded embedding through a dropout layer\n        x = self.dropout(x, training=training)\n        \n        # Pass the output through the stack of encoding layers \n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x,training, mask)\n\n        return x  # Shape: (batch_size, input_seq_len, embedding_dim)","metadata":{"id":"7j2Tjr0K0t0I"},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### 5 - Decoder\n\nTransformer Decoder layer takes K and V matrices generated by Encoder and computes the second MHA layer with Q matrix from the output (Fig. 3a).\n\n<img src=\"images/decoder_layer.png\" alt=\"Encoder\" width=\"250\"/>\n<caption><center><font color='purple'><b>Figure 3a: Transformer Decoder layer</font></center></caption>\n \n### 5.1 - Decoder Layer\nAgain, the MHA is paired with a FFNN, but this time 2 MHA layers are implemented. Residual connections and layer normalization are again used (Fig. 3a).\n    \n `DecoderLayer()` is implemented using `call()` method. \n    \n1. Block 1 is MHA layer with residual connection, dropout layer, and look-ahead mask.\n2. Block 2 will take into account the output of the Encoder, so MHA layer will receive K and V from the encoder, and Q from  Block 1. Dropout layer, layer normalization and residual connection are applied, like before. \n3. Block 3 is FFNN with dropout and normalization layers and a residual connection.\n    \n**Additional Hints:**\n* The first two blocks are fairly similar to the EncoderLayer except you will return `attention_scores` when computing self-attention. ","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(tf.keras.layers.Layer):\n    \"\"\"\nDecoder layer is composed by two MHA blocks, \none that takes the new input and uses self-attention, and the other \none that combines it with the output of the encoder, followed by a\nfully connected block. \n    \"\"\"\n    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = ?\n        \n        self.mha2 = ?\n\n        self.ffn =  ?\n\n        self.layernorm1 = ?\n        self.layernorm2 = ?\n        self.layernorm3 = ?\n\n        self.dropout1 = ?\n        self.dropout2 = ?\n        self.dropout3 = ?\n    \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        \"\"\"\n        Forward pass for the Decoder Layer\n        \nArguments:\nx: Tensor of shape (batch_size, target_seq_len, embedding_dim)\nenc_output:  Tensor of shape(batch_size, input_seq_len, embedding_dim)\ntraining: Boolean, set to true to activate training mode for dropout layers\nlook_ahead_mask: Boolean mask for the target_input\npadding_mask:  Boolean mask for the 2nd MHA layer\n\nReturns:\nout3: Tensor of shape (batch_size, target_seq_len, embedding_dim)\nattn_weights_block1: Tensor of shape(batch_size, num_heads, \n                               target_seq_len, input_seq_len)\nattn_weights_block2: Tensor of shape (batch_size, num_heads, \n                            target_seq_len, input_seq_len)\n        \"\"\"\n        \n# BLOCK 1\n# calculate self-attention and return attention scores as \n# attn_weights_block1\n        attn1, attn_weights_block1 = self.mha1(x, x, x,look_ahead_mask, return_attention_scores=True)  # (batch_size, target_seq_len, d_model)\n        \n# apply 1st dropout layer on the attn1 output \n        attn1 = ?\n        \n#apply 1st layer normalization to the sum of attn1 output & input x\n        out1 = ?\n\n# BLOCK 2\n# calculate self-attention using Q from the 1st block and K and V\n#  from the encoder output.\n# MultiHeadAttention's call takes input (Query, Value, Key, \n# attention_mask, return_attention_scores, training)\n# Return attention scores as attn_weights_block2 \n        attn2, attn_weights_block2 = self.mha2( out1,enc_output, enc_output, padding_mask, return_attention_scores=True)  # (batch_size, target_seq_len, d_model)\n        \n        # apply 2nd dropout layer on the attn2 output\n        attn2 = ?\n        \n# apply 2nd layer normal. to the sum of attn2 output & output of Block 1\n#  out2 shape: (batch_size, target_seq_len, embedding_dim)\n        out2 = ? \n        \n #BLOCK 3\n        # pass the output of the 2nd block through a ffn\n        # ffn_output shape: (batch_size, target_seq_len, embedding_dim)\n        ffn_output = ? \n        \n        # apply 3rd dropout layer to ffn output\n        ffn_output = ?\n        \n# Apply 3rd layer normaliz. to the sum of ffn output & output of Block 2\n# Output shape:(batch_size, target_seq_len, embedding_dim)\n        out3 =  ?\n\n\n        return out3, attn_weights_block1, attn_weights_block2\n    ","metadata":{"id":"wEouNFvCzMeT"},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Full Decoder\n\n<img src=\"images/decoder.png\" alt=\"Encoder\" width=\"300\"/>\n<caption><center><font color='purple'><b>Figure 3b: Transformer Decoder</font></center></caption>\n\nImplement `Decoder()` using `call()` method to embed the output, add positional encoding, and implement multiple decoder layers. \n \nThe Decoder is initialized with an Embedding layer, positional encoding, and multiple DecoderLayers. `call()` method performs the following steps: \n1. Pass the generated output through the Embedding layer.\n2. Scale the embedding by multiplying it by the square root of the embedding dimension. Cast the embedding dimension to data type `tf.float32` before computing the square root.\n3. Add the position encoding: self.pos_encoding `[:, :seq_len, :]` to the embedding.\n4. Pass the encoded embedding through a dropout layer. Use the `training` parameter to set the model training mode. \n5. Pass the output of the dropout layer through the stack of Decoding layers using a for loop.","metadata":{}},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n    \"\"\"\nThe entire Encoder starts by passing the target input to an embedding\nlayer and using positional encoding to then pass the output through a \nstack of decoder Layers\n        \n    \"\"\" \n    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, target_vocab_size,\n               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(Decoder, self).__init__()\n\n        self.embedding_dim = embedding_dim\n        self.num_layers = num_layers\n\n        self.embedding = Embedding(target_vocab_size, self.embedding_dim)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n\n        self.dec_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n                                        num_heads=num_heads,\n                                        fully_connected_dim=fully_connected_dim,\n                                        dropout_rate=dropout_rate,\n                                        layernorm_eps=layernorm_eps) \n                           for _ in range(self.num_layers)]\n        self.dropout = Dropout(dropout_rate)\n    \n    def call(self, x, enc_output, training, \n           look_ahead_mask, padding_mask):\n        \"\"\"\n        Forward  pass for the Decoder\n        \nArguments:\nx -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\nenc_output --  Tensor of shape(batch_size, input_seq_len, embedding_dim)\ntraining -- Boolean, set to true to activate the training mode \nfor dropout layers\nlook_ahead_mask -- Boolean mask for the target_input\npadding_mask -- Boolean mask for the second multihead attention layer\n\nReturns:\nx -- Tensor of shape (batch_size, target_seq_len, embedding_dim)\nattention_weights - Dictionary of tensors containing all attention \nweights as Tensor of shape \n(batch_size, num_heads, target_seq_len, input_seq_len)\n        \"\"\"\n\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n        \n        # Pass input through the Embedding layer \n        # x shape: (batch_size, target_seq_len, embedding_dim)\n        x = ?\n        \n# Scale embeddings multiplying by square root of embeddings dimension\n        x *= ?\n        \n        # Add positional encodings to word embedding\n        x += ?\n        \n        # apply a dropout layer to x\n        x = ?\n\n# Pass x and the encoder output through a stack of decoder layers and \n# save the attention weights of block1 and block2\n\n        for i in range(self.num_layers):\n\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n\n#update attention_weights dictionary with the attention weights of \n# block 1 and block 2\n            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = ?\n            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = ?\n        \n        # x.shape = > (batch_size, target_seq_len, embedding_dim)\n        return x, attention_weights","metadata":{"id":"McS3by6k4pnP"},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### 6 - Transformer\n<img src=\"images/transformer.png\" alt=\"Transformer\" width=\"550\"/>\n<caption><center><font color='purple'><b>Figure 4: Transformer</font></center></caption>\n    \nThe flow of data through the Transformer Architecture is as follows:\n* Input passes through the implemented above Encoder:\n    - embedding and positional encoding of the input\n    - MHA on the input\n    - FFNN to detect features\n* The encoder output passes through  the implemented above Decoder:\n    - embedding and positional encoding of the output\n    - MHA on the generated output\n    - MHA with Q from the 1st MHA layer and K and V from the Encoder. \n    - FFNN to help detect features\n* Finally, after the Nth Decoder layer, two dense layers and softmax are applied to generate prediction for the next output in the sequence.\n\n`Transformer()` is implemented using `call()` method:\n1. Pass input through the Encoder with appropiate mask.\n2. Pass encoder output and target through Decoder with appropiate mask.\n3. Apply linear transformation and softmax to get prediction.","metadata":{}},{"cell_type":"code","source":"class Transformer(tf.keras.Model):\n    \"\"\"\n    Complete transformer with an Encoder and a Decoder\n    \"\"\"\n    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n               target_vocab_size, max_positional_encoding_input,\n               max_positional_encoding_target, dropout_rate=0.1, layernorm_eps=1e-6):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers=num_layers,\n                               embedding_dim=embedding_dim,\n                               num_heads=num_heads,\n                            fully_connected_dim=fully_connected_dim,\n                               input_vocab_size=input_vocab_size,\n            maximum_position_encoding=max_positional_encoding_input,\n                               dropout_rate=dropout_rate,\n                               layernorm_eps=layernorm_eps)\n\n        self.decoder = Decoder(num_layers=num_layers, \n                               embedding_dim=embedding_dim,\n                               num_heads=num_heads,\n                            fully_connected_dim=fully_connected_dim,\n                               target_vocab_size=target_vocab_size, \n            maximum_position_encoding=max_positional_encoding_target,\n                               dropout_rate=dropout_rate,\n                               layernorm_eps=layernorm_eps)\n\n        self.final_layer = Dense(target_vocab_size, activation='softmax')\n    \n    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n        \"\"\"\n        Forward pass for the entire Transformer\nArguments:\ninp -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\ntar -- Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\ntraining -- Boolean, set to true to activate\n                        the training mode for dropout layers\nenc_padding_mask -- Boolean mask to ensure that the padding is not \n                    treated as part of the input\nlook_ahead_mask -- Boolean mask for the target_input\npadding_mask -- Boolean mask for the 2nd MHA layer\n\nReturns:\nfinal_output --\nattention_weights - Dictionary of tensors containing all attention \nweights for the decoder each Tensor of shape \n(batch_size, num_heads, target_seq_len, input_seq_len)\n        \n        \"\"\"\n# call self.encoder with appropriate arguments to get encoder output\n# Output shape: (batch_size, inp_seq_len, fully_connected_dim)\n        enc_output = self.encoder(inp,training,enc_padding_mask) \n        \n# call self.decoder with appropriate arguments to get decoder output\n# Output shape: (batch_size, tar_seq_len, fully_connected_dim)\n        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n        \n# pass decoder output through a linear layer and softmax\n# Output shape: (batch_size, tar_seq_len, target_vocab_size)\n        final_output = self.final_layer(dec_output)  \n\n        return final_output, attention_weights","metadata":{"id":"QHymPmaj-2ba"},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"<font color='blue'>\n    <b>What you should remember</b>:\n\n- Combination of self-attention and FFNN layers allows for parallization of training and *faster training*.\n- Self-attention is calculated using the generated query Q, key K, and value V matrices.\n- Adding positional encoding to word embeddings is an effective way to include sequence information in self-attention calculations. \n- Multi-head attention (MHA) helps to detect multiple features in the sentence.\n- Masking stops the model from 'looking ahead' during training, or weighting zeroes too much when processing cropped sentences. ","metadata":{}},{"cell_type":"markdown","source":"### 7 - References\n\nThe Transformer algorithm was due to Vaswani et al. (2017). \n\n- Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin (2017). [Attention Is All You Need](https://arxiv.org/abs/1706.03762) ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}