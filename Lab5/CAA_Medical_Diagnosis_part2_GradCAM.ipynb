{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZYK-0rin5x7"
   },
   "source": [
    "### LAB: Chest X-Ray Images - Medical Diagnosis with DenseNet (part 2) \n",
    "\n",
    "**Objectives:** Build a Convolutonal Neural Network (CNN) classifier to discriminate between 14 different pathologies from chest X-ray images using functions from Keras framework. \n",
    "\n",
    " In particular, you will:\n",
    "- Pre-process and prepare a real-world X-ray dataset\n",
    "- Use a pretrained DenseNet CNN model for X-ray image classification\n",
    "- Learn a technique to handle class imbalance\n",
    "- Evaluate the model performance with test data\n",
    "- Explain the model activity with Class Activation Maps (Grad-CAM).\n",
    "\n",
    "<img src=\"images/xray-header-image.png\" style=\"width:600px;height:200px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Je3yV0Wnn5x8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# for numerical manipulation\n",
    "import numpy as np \n",
    "\n",
    "#for analyzing and manipulating data (https://pandas.pydata.org/docs/#)\n",
    "import pandas as pd\n",
    "\n",
    "#high-level interface for drawing attractive and informative \n",
    "#statistical graphics\n",
    "import seaborn as sns\n",
    "\n",
    " #to produce plots for visualization\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "from keras.models import load_model\n",
    "\n",
    "# util provides locally defined utility functions required for this lab work.\n",
    "import util\n",
    "\n",
    "%matplotlib inline\n",
    "tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PMDCWQRn5yA"
   },
   "source": [
    "###  1. Load Chest X-ray Images: Data Exploration and Image Preprocessing \n",
    "\n",
    "**Objectives**: Become familiar with Chest X-ray data set, that is a small part of the images in the  [ChestX-ray8 dataset](https://arxiv.org/abs/1705.02315) which contains 108,948 frontal-view X-ray images of 32,717 patients. \n",
    "\n",
    "The first step for any Machine Learning (ML) project is to explore the data.\n",
    "\n",
    "- Each image in the data set contains labels for 14 different pathological conditions, that can be used by physicians to diagnose different diseases. \n",
    "- We will use this data to develop a binary classification model to predict 'positive' (pathology is present, label 1) or 'negative' (pathology is not present, label 0) for each of the 14 labeled pathologies. \n",
    "- The dataset consists of  422 images stored in folder **nih/images-small/**. \n",
    "- The dataset includes 3 CSV (Comma-Separated Values) files with the labels for each X-ray: \n",
    "\n",
    "1. `nih/train-small.csv`: training images.\n",
    "1. `nih/valid-small.csv`: validation images.\n",
    "1. `nih/test.csv`:testing images . \n",
    "\n",
    "#### Several meanings of word  'class'\n",
    "The word **'class'** is used in multiple ways is this lab. \n",
    "- We sometimes refer to each of the 14 pathological conditions that are labeled in our dataset as a class. \n",
    "- But for each of those pathologies we are attempting to predict whether a certain condition is present (i.e. positive result) or absent (i.e. negative result). \n",
    "    - These two possible labels of 'positive' or 'negative' (or the numerical equivalent of 1 or 0) are also typically referred to as classes. \n",
    "- Moreover, we also use the term in reference to software code 'classes' such as `ImageDataGenerator`.\n",
    "\n",
    "As long as you are aware of all this though, it should not cause you any confusion as the term 'class' is usually clear from the context in which it is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "5JRSHB7i0t_6",
    "outputId": "69830050-af47-4ebc-946d-d411d0cbdf5b"
   },
   "outputs": [],
   "source": [
    "# Read the csv file containing training data with pandas library \n",
    "train_df = ?\n",
    "\n",
    "# Print the first few rows of train_df => dictionary with several keys \n",
    "? \n",
    "\n",
    "#What are the keys ?\n",
    "\n",
    "#What are the dimensions (rows, columns) of train_df and their meaning ? \n",
    "?\n",
    "\n",
    "# Do the same for validation and testing data\n",
    "\n",
    "valid_df = ?\n",
    "\n",
    "test_df = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrDoMlsun5yE"
   },
   "outputs": [],
   "source": [
    "# 14 pathological conditions \n",
    "\n",
    "labels = ['Cardiomegaly', \n",
    "          'Emphysema', \n",
    "          'Effusion', \n",
    "          'Hernia', \n",
    "          'Infiltration', \n",
    "          'Mass', \n",
    "          'Nodule', \n",
    "          'Atelectasis',\n",
    "          'Pneumothorax',\n",
    "          'Pleural_Thickening', \n",
    "          'Pneumonia', \n",
    "          'Fibrosis', \n",
    "          'Edema', \n",
    "          'Consolidation']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data labels\n",
    "The csv file has 16 columns: \"Image\" column with the names of the chest x-ray images, \"PatientId\" column with the patient ID and 14 other (binary) columns filled with 1 or 0 to identify if the pathology is present (1) or not (0) in this x-ray image.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the # of positive labels (1's) for each condition (pathology).\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are supposed to see that data is highly unbalanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Visualization - Display some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imag = []\n",
    "imag.append('00008270_015.png')\n",
    "imag.append('00000116_034.png')\n",
    "imag.append(\"00000359_010.png\")\n",
    "imag.append(\"00000121_008.png\")\n",
    "imag.append('00000003_001.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the image directory\n",
    "IMAGE_DIR = \"nih/images-small/\"\n",
    "\n",
    "print('Display some images')\n",
    "\n",
    "# Adjust the size of the images\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i in range(3):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    img = plt.imread(os.path.join(IMAGE_DIR, imag[i]))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate a single image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the first image from imag\n",
    "sample_img = imag[0]\n",
    "\n",
    "#  Image dimension ? \n",
    "#Answer: 1024 px width, 1024 px height, 1 color (gray) channel\n",
    "\n",
    "?\n",
    "\n",
    "# Pixel max, min, mean values, standard deviation \n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate pixel value distribution\n",
    "Plot the distribution of pixel values in the image you selected above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    " ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JBWZ5l4ln5yH"
   },
   "source": [
    "###  3. Preparing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPjuZHPpn5yH"
   },
   "source": [
    "- We will use now [ImageDataGenerator](https://keras.io/preprocessing/image/) class from Keras framework, which allows to build a \"generator\" for images specified in a dataframe. \n",
    "- This class also provides support for basic data augmentation such as random horizontal flipping of images.\n",
    "- We also use the generator to normalize the values in each batch (train, validate, test sets) so that their mean is $0$ and their standard deviation is 1. In other words, the generator will replace each pixel value in the image with a new value calculated by subtracting the mean and dividing by the standard deviation.  This will facilitate model training by standardizing the input distribution. \n",
    "\n",
    "$$\\frac{x_i - \\mu}{\\sigma}$$\n",
    "\n",
    "- The generator also converts a single channel X-ray images (gray-scale) to a 3-channel format by repeating the values in the image across all channels, because the pre-trained model that we'll use requires 3-channel inputs.\n",
    "\n",
    "The implemented generator below will\n",
    "1. Normalize the mean and standard deviation of the data\n",
    "3. Shuffle the input after each epoch.\n",
    "4. Set the image size to be 320px by 320px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAgVGOAju8pX"
   },
   "outputs": [],
   "source": [
    "def get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, \n",
    "    batch_size=8, seed=1, target_w = 320, target_h = 320):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return generator for training set, normalizing using batch statistics.\n",
    "\n",
    "    Args:\n",
    "      train_df (dataframe): dataframe specifying training data.\n",
    "      image_dir (str): directory where image files are held.\n",
    "      x_col (str): name of column in df that holds filenames.\n",
    "      y_cols (list): list of strings that hold y labels for images.\n",
    "      batch_size (int): images per batch to be fed into model during training.\n",
    "      seed (int): random seed.\n",
    "      target_w (int): final width of input images.\n",
    "      target_h (int): final height of input images.\n",
    "    \n",
    "    Returns:\n",
    "        train_generator (DataFrameIterator): that transforms the entire training set\n",
    "    \"\"\"        \n",
    "    print(\"Train images:\")\n",
    "    # normalize images\n",
    "    image_generator = ImageDataGenerator(\n",
    "        samplewise_center=True,\n",
    "        samplewise_std_normalization= True)\n",
    "    \n",
    "    # flow from directory with specified batch size\n",
    "    # and target image size\n",
    "    generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vpRXR-3_u7cl"
   },
   "source": [
    "Now we build a new generator to transform (normalize) the validation and testing data. \n",
    "We can't use the same generator as for the training data because the test data has to be normalized using the statistics computed from the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UtWEAfAnrhMq"
   },
   "outputs": [],
   "source": [
    "def get_test_and_valid_generator(valid_df, test_df, train_df, image_dir,\n",
    "    x_col, y_cols, sample_size=100, batch_size=8, seed=1, \n",
    "    target_w = 320, target_h = 320):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return generator for validation set and test test set using \n",
    "    normalization statistics from training set.\n",
    "\n",
    "    Args:\n",
    "      valid_df (dataframe): dataframe specifying validation data.\n",
    "      test_df (dataframe): dataframe specifying test data.\n",
    "      train_df (dataframe): dataframe specifying training data.\n",
    "      image_dir (str): directory where image files are held.\n",
    "      x_col (str): name of column in df that holds filenames.\n",
    "      y_cols (list): list of strings that hold y labels for images.\n",
    "      sample_size (int): size of sample to use for normalization \n",
    "      statistics.\n",
    "      batch_size (int): images per batch to be fed into model during \n",
    "      training.\n",
    "      seed (int): random seed.\n",
    "      target_w (int): final width of input images.\n",
    "      target_h (int): final height of input images.\n",
    "    \n",
    "    Returns:\n",
    "        test_generator (DataFrameIterator) and valid_generator: \n",
    "        iterators over test set and validation set respectively\n",
    "    \"\"\"\n",
    "     # Generator of train images\n",
    "    print(\"Train images:\")\n",
    "    # get generator to sample dataset\n",
    "    raw_train_generator = ImageDataGenerator().flow_from_dataframe(\n",
    "        dataframe=train_df, \n",
    "        directory=IMAGE_DIR, \n",
    "        x_col=\"Image\", \n",
    "        y_col=labels, \n",
    "        class_mode=\"raw\", \n",
    "        batch_size=sample_size, \n",
    "        shuffle=True, \n",
    "        target_size=(target_w, target_h))\n",
    "    \n",
    "    # get data sample (1 image) \n",
    "    batch = raw_train_generator.next()\n",
    "    data_sample = batch[0]\n",
    "\n",
    "    # use 1 sample (1 image) to fit mean and std for test set generator\n",
    "    image_generator = ImageDataGenerator(\n",
    "        featurewise_center=True,\n",
    "        featurewise_std_normalization= True)\n",
    "    \n",
    "    # fit generator to sample from training data\n",
    "    image_generator.fit(data_sample)\n",
    "\n",
    "    # Generator of validation images\n",
    "    print(\"Validation images:\")\n",
    "    valid_generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=valid_df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "\n",
    "    # Generator of test images\n",
    "    print(\"Test images:\")\n",
    "    test_generator = image_generator.flow_from_dataframe(\n",
    "            dataframe=test_df,\n",
    "            directory=image_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_cols,\n",
    "            class_mode=\"raw\",\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            seed=seed,\n",
    "            target_size=(target_w,target_h))\n",
    "    return valid_generator, test_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga4RZN5On5yL"
   },
   "source": [
    "Apply the generator function to preprocess the training, validation and test data\n",
    "\n",
    "**Note:** The train_df csv file has a list of 1000 images, however in the directory we only have 296 training images, therefore it prints a warning message. and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "rNE3HWRbn5yL",
    "outputId": "4c6b1c25-a33d-42e0-f442-40971ca52a3f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the generators to prepare the training, validation and test data\n",
    "\n",
    "train_generator = ?\n",
    "\n",
    "valid_generator, test_generator= ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pYtXacDgn5yN"
   },
   "source": [
    "Let's see what the generator gives to the model by `__getitem__(index)` function and compare with the original image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "Jh77vpN-n5yO",
    "outputId": "c4e68e79-e8f2-4bb9-8909-072c9dd2f805"
   },
   "outputs": [],
   "source": [
    "generated_image, label = train_generator.__getitem__(0)\n",
    "\n",
    "# What is the shape of the generated_image ? \n",
    "?\n",
    "# What is the meaning of each dimension ?\n",
    "\n",
    "# What is the pixel values range (max, min) of the normalized images ?\n",
    "?\n",
    "\n",
    "# What is the Mean pixel value and the standard deviation ?\n",
    "\n",
    "?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first image from generated_image (normalized image)\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first image from imag (original image)\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histograms of the original and the normalized images and get a similar figure.\n",
    "\n",
    "<img src=\"images/f1.png\" style=\"width:300px;height:200px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of the distribution of the pixels\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9WBMpRxcDMgp"
   },
   "source": [
    "### 4. Addressing Class Imbalance\n",
    "One of the challenges with medical datasets is the large class imbalance. Let's plot the frequency of each of the labels in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the class frequency\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "sns.barplot(labels, np.mean(train_generator.labels, axis=0), color='b')\n",
    "#sns.barplot(np.mean(train_generator.labels, axis=0), labels, color='b')\n",
    "plt.title(\"Frequency of Each Class\", fontsize=10)\n",
    "plt.ylabel('Pathology', fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# What is the dimension and the meaning of train_generator.labels ?\n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3nHRd9p9n5yU"
   },
   "source": [
    "We can see from this plot that the presence of different pathologies (classes) varies significantly. (The same trend happens in the full dataset as well.) \n",
    "* The `Hernia` pathology has the greatest imbalance with the proportion of positive training cases being about 0.2%. \n",
    "* Even the `Infiltration` pathology, has about 15.5% of the training cases labelled positive.\n",
    "\n",
    "If we use a normal cross-entropy loss function with this highly unbalanced dataset, the algorithm will be incentivized to prioritize the majority class (i.e negative in our case), since it contributes more to the loss. \n",
    "\n",
    "#### Impact of class imbalance on loss function\n",
    "\n",
    " We will use a cross-entropy loss for each pathology. The normal cross-entropy loss from the $i^{th}$ training example is:\n",
    "\n",
    "$$\\mathcal{L}_{cross-entropy}(x_i) = -(y_i \\log(f(x_i)) + (1-y_i) \\log(1-f(x_i))),$$\n",
    "\n",
    "where $x_i$ are the input features and $y_i$ is the label, and $f(x_i)$ is the output of the model, i.e. the probability that it is positive (1) or negative (0). \n",
    "\n",
    "For any training case, either $y_i=0$ or $y_i=1$ , so only one term contributes to the loss (the other term is multiplied by zero). \n",
    "\n",
    "The average cross-entropy loss over the entire training set $\\mathcal{D}$ of size $N$ is as follows: \n",
    "\n",
    "$$\\mathcal{L}_{cross-entropy}(\\mathcal{D}) = - \\frac{1}{N}\\big( \\sum_{\\text{positive examples}} \\log (f(x_i)) + \\sum_{\\text{negative examples}} \\log(1-f(x_i)) \\big).$$\n",
    "\n",
    "We can see that if there is a large imbalance with very few positive training cases, then the loss will be dominated by the negative class. Summing the contribution over all the training cases for each class (i.e. pathological condition), we see that the contribution of each class (i.e. positive or negative) is: \n",
    "\n",
    "$$freq_{p} = \\frac{\\text{number of positive examples}}{N} $$\n",
    "\n",
    "$$\\text{and}$$\n",
    "\n",
    "$$freq_{n} = \\frac{\\text{number of negative examples}}{N}.$$\n",
    "\n",
    "\n",
    "### Computing Class Frequencies\n",
    "Complete the function below to calculate these frequences for each label in the dataset.\n",
    "\n",
    "Use numpy.sum(a, axis= ?), and choose the axis (0 or 1) </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TpDGeY2cChYD"
   },
   "outputs": [],
   "source": [
    "def compute_class_freqs(labels):\n",
    "    \"\"\"\n",
    "    Compute positive and negative frequences for each class.\n",
    "\n",
    "    Args:\n",
    "        labels (np.array): matrix of labels, size (num_examples, \n",
    "        num_classes)\n",
    "        \n",
    "    Returns:\n",
    "positive_frequencies (np.array): array of positive frequences for each\n",
    "                                class, size (num_classes)\n",
    "negative_frequencies (np.array): array of negative frequences for each\n",
    "                                class, size (num_classes)\n",
    "    \"\"\"\n",
    "    \n",
    "    # total number of patients (rows)\n",
    "    N = ?\n",
    "    \n",
    "    positive_frequencies = ?\n",
    "    negative_frequencies = ?\n",
    "\n",
    "    return positive_frequencies, negative_frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Iye-sQoOFG37"
   },
   "source": [
    "Compute the frequencies for our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LoxM5jQ0E30D"
   },
   "outputs": [],
   "source": [
    "freq_pos, freq_neg = ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gsJIDPTZn5yW"
   },
   "source": [
    "Let's visualize these two contribution ratios next to each other for each of the pathologies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "IqnNCu4In5yW",
    "outputId": "245f1a6b-b292-4c6d-a583-c6924bc61f31",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": \n",
    "                     freq_pos})\n",
    "\n",
    "data = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": \n",
    "    v} for l,v in enumerate(freq_neg)], ignore_index=True)\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "f = sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2uvttCM8n5yY"
   },
   "source": [
    "As we see in the above plot, the contributions of positive cases is significantly lower than that of the negative ones. However, we want the contributions to be equal. One way of doing this is by multiplying each example from each class by a class-specific weight factor, $w_{pos}$ and $w_{neg}$, so that the overall contribution of each class is the same. \n",
    "\n",
    "\n",
    "$$pos\\_contribution=w_{pos} \\times freq_{p} = w_{neg} \\times freq_{n} =neg\\_contribution$$\n",
    "\n",
    "$$w_{pos} = freq_{neg}$$\n",
    "$$w_{neg} = freq_{pos}$$\n",
    "\n",
    "This way, we will be balancing the contribution of positive and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zs3_Rgwwn5yZ"
   },
   "outputs": [],
   "source": [
    "pos_w = ?\n",
    "neg_w = ?\n",
    "pos_contribution = ?\n",
    "neg_contribution = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygNZmdyun5ya"
   },
   "source": [
    "Verify that now the contribution of the positive and negative labels is equal. It is expected to get a similar plot as the one below. \n",
    "\n",
    "<img src=\"images/f_bal_contr.png\" style=\"width:300px;height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "id": "LPfSFrxjn5yb",
    "outputId": "a4b6354f-ab39-4623-d44b-90cfd9b28506",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "f = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9xgoEkpn5yc"
   },
   "source": [
    "As the above figure shows, by applying these weightings the positive and negative labels within each class would have the same aggregate contribution to the loss function. \n",
    "After computing the weights, the final weighted loss for each training case will be \n",
    "\n",
    "$$\\mathcal{L}_{cross-entropy}^{w}(x) = - (w_{p} y \\log(f(x)) + w_{n}(1-y) \\log( 1 - f(x) ) ).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the Weighted Loss\n",
    "The function below computes the average weighted loss for all training examples (batch data). For the multi-class loss, the average loss for each individual class is computed. The small value, $\\epsilon$, is added to the predicted values before taking their logs. This is to avoid a numerical error that would otherwise occur if the predicted value happens to be zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPIBVAasn5yd"
   },
   "outputs": [],
   "source": [
    "def get_weighted_loss(pos_weights, neg_weights, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Return weighted loss function given negative weights and \n",
    "    positive weights.\n",
    "\n",
    "    Args:\n",
    "      pos_weights (np.array): array of positive weights for each class, size (num_classes)\n",
    "      neg_weights (np.array): array of negative weights for each class, size (num_classes)\n",
    "      epsilon: to avoid numerical errors if the predicted value = 0.\n",
    "    \n",
    "Returns:\n",
    "      weighted_loss (function): weighted loss function\n",
    "    \"\"\"\n",
    "    def weighted_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Return weighted loss value. \n",
    "\n",
    "Args:\n",
    " y_true (Tensor): Tensor of true labels, size is \n",
    " (num_examples, num_classes)\n",
    " y_pred (Tensor): Tensor of predicted labels, size is (num_examples, \n",
    " num_classes)\n",
    " \n",
    "Returns:\n",
    "    loss (Float): overall scalar loss summed across all classes\n",
    "        \"\"\"\n",
    "        # initialize loss to zero\n",
    "        loss = 0.0\n",
    "        \n",
    "        for i in range(len(pos_weights)):\n",
    "# for each class, add average weighted loss for that class\n",
    "#K.mean, K.log are Keras functions to calculate the mean and the log.\n",
    "            \n",
    "            loss_pos = -1 * K.mean(pos_weights[i] * y_true[:, i] * K.log(y_pred[:, i] + epsilon))\n",
    "            loss_neg = -1 * K.mean(neg_weights[i] * (1 - y_true[:, i]) * K.log(1 - y_pred[:, i] + epsilon))\n",
    "            \n",
    "            loss += loss_pos + loss_neg \n",
    "        return loss\n",
    "    \n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. DenseNet121 Model \n",
    "\n",
    "\n",
    "DenseNet is a deep Convolutional Neural Network (CNN) where each layer is connected to all other layers that are deeper in the network. \n",
    "- The first layer is connected to the 2nd, 3rd, 4th etc.\n",
    "- The second layer is connected to the 3rd, 4th, 5th etc. Like shown in the figure.\n",
    "\n",
    "For a detailed explanation of Densenet, check out the paper by Gao Huang et al. 2018 [Densely Connected Convolutional Networks](https://arxiv.org/pdf/1608.06993.pdf).\n",
    "\n",
    "We will use a pre-trained [DenseNet121](https://www.kaggle.com/pytorch/densenet121) model, loaded from Keras, and add 2 layers on top of it:\n",
    "1. A `GlobalAveragePooling2D` layer to get the average of the last convolution layers from DenseNet121.\n",
    "2. A `Dense` layer with `sigmoid` activation to get the predictions for each of the classes.\n",
    "\n",
    "<img src=\"nih/densenet.png\" alt=\"U-net Image\" width=\"300\" align=\"middle\"/>«\n",
    "\n",
    "We can set our custom loss function for the model by specifying the `loss` parameter in the `compile()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base DenseNet121 model with pretrained weights stored \n",
    "# in file densenet.hdf5\n",
    "\n",
    "base_model = DenseNet121(weights='./nih/densenet.hdf5', include_top=False)\n",
    "\n",
    "x = base_model.output\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# and a logistic layer\n",
    "predictions = Dense(len(labels), activation=\"sigmoid\")(x)\n",
    "\n",
    "# Create an updated model with the added layers \n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model (adam optimizer, \n",
    "# for loss instead of categorical_crossentropy specify the\n",
    "# custom loss function get_weighted_loss )\n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "\n",
    "?\n",
    "\n",
    "# How many Total params ? (answer: 7,037,504 )\n",
    "# Trainable params ? (answer: 6,953,856 )\n",
    "# Non-trainable params ? (answer:  83,648 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the first five layers\n",
    "print(\"First 5 layers\")\n",
    "\n",
    "?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the last five layers\n",
    "print(\"Last 5 layers\")\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the CONVOLUTIONAL layers and print the first 5\n",
    "\n",
    "print(\"The first five conv2D layers\")\n",
    " ?\n",
    "\n",
    "# Print out the total number of convolutional layers \n",
    "#(ANSWER: 120 convolutional layers)\n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the dimension of the input ? How many channels in the input?\n",
    "\n",
    "base_model.input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Training [optional]\n",
    "\n",
    "With the model ready for training, we can use the `model.fit()` function in Keras to train the classifier model. \n",
    "Since training can take a considerable time, we have chosen not to train the model here but rather to load a set of pre-trained weights in the next section. However, you can use the code shown below to practice training the model locally on your machine or in Colab.\n",
    "\n",
    "Python Code for training the model:\n",
    "\n",
    "```python\n",
    "history = model.fit_generator(train_generator, \n",
    "                              validation_data=valid_generator,\n",
    "                              steps_per_epoch=100, \n",
    "                              validation_steps=25, \n",
    "                              epochs = 3)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained weights into the model:\n",
    "\n",
    "model.load_weights(\"./nih/pretrained_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Prediction and Evaluation\n",
    "Now that we have a model, let's evaluate it with the test set. \n",
    "\n",
    "**Note:** Depending on your computer, the following cell may take some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_vals = model.predict(test_generator, \n",
    "                               steps = len(test_generator))\n",
    "\n",
    "# what is the dimension and the meaning of predicted_vals ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The error between the predicted class and the given labels \n",
    "#(the ground truth)\n",
    "\n",
    "e= ?\n",
    "\n",
    "#the test error (in %) per class\n",
    "em = ?\n",
    "\n",
    "# the mean test error (ANSWER: 0.27)\n",
    " ?\n",
    "\n",
    "#Plot the test error per class \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and visualize train & validation errors per class. \n",
    "\n",
    "Mean train error = 0.277\n",
    "\n",
    "Mean validation error = 0.262\n",
    "\n",
    "Mean test error = 0.27\n",
    "\n",
    "Get similur results as the figures below. \n",
    "\n",
    "<img src=\"images/f2.jpg\" style=\"width:300px;height:300px;\">\n",
    "\n",
    "<img src=\"images/f3.jpg\" style=\"width:300px;height:300px;\">\n",
    "\n",
    "<img src=\"images/f4.jpg\" style=\"width:300px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 Visualizing Learning with GradCAM \n",
    "\n",
    "One of the challenges of using deep learning in medicine is that the complex architecture used for neural networks makes them harder to interpret. \n",
    "\n",
    "A common approach to increase the interpretability of computer vision models is to use Class Activation Maps (CAM). \n",
    "- Class activation maps are useful for understanding where the model is \"looking\" when classifying an image. \n",
    "\n",
    "Here we use a [GradCAM's](https://arxiv.org/abs/1610.02391) technique to produce a heatmap highlighting the important regions in the image for predicting the pathological condition. \n",
    "- This is done by extracting the gradients of each predicted class, flowing into the model's final convolutional layer. Look at the `util.compute_gradcam` which has been provided for you in `util.py` to see how this is done with the Keras framework. \n",
    "\n",
    "GradCAM does not provide a full explanation of the reasoning for each classification probability. \n",
    "- However, it is still a useful tool so that an expert could validate that a prediction is indeed due to the model focusing on the right regions of the image.\n",
    "\n",
    "Let's look at the first 4 patologies of one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"nih/train-small.csv\")\n",
    "IMAGE_DIR = \"nih/images-small/\"\n",
    "\n",
    "util.compute_gradcam(model, '00000003_001.png', IMAGE_DIR, train_df, labels, labels[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate the heatmap for the other classes of the same image, \n",
    "only 4 patologies at once.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "G5aZAlVbn5yz"
   ],
   "include_colab_link": true,
   "name": "C1M2_Assignment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "coursera": {
   "schema_names": [
    "AI4MC1-1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
