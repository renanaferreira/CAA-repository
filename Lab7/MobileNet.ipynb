{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab: Transfer Learning with MobileNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab work, you will use a pre-trained MobileNetV2 to build a binary classifier (with images of Alpaca/Not Alpaca animal). \n",
    "MobileNetV2, was designed to provide fast and computationally efficient performance. It is pre-trained on ImageNet, a dataset containing over 14 million images and 1000 classes.\n",
    "\n",
    "<img src=\"images/alpaca.png\" style=\"width:300px;height:220px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
    "\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation data Sets\n",
    "Keras function `image_data_set_from_directory()` reads from the directory and create training and validation datasets. \n",
    "The seed need to be the same, so the training and validation sets don't overlap.  \n",
    "\n",
    "If the directory structure is:\n",
    "\n",
    "main_directory/\n",
    "\n",
    "...class_a/ (in our case directory alpaca => 142 images )\n",
    "\n",
    "...class_b/  (in our case directory nonalpaca => 185 images )\n",
    "\n",
    "Then calling `image_data_set_from_directory(main_directory, labels='inferred')` will return a tf.data.Dataset that yields batches of images from the subdirectories class_a and class_b, together with labels 0 and 1 (0 corresponding to class_a and 1 corresponding to class_b).\n",
    "\n",
    "Supported image formats: jpeg, png, bmp, gif. Animated gifs are truncated to the first frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 327 files belonging to 2 classes.\n",
      "Using 262 files for training.\n",
      "Found 327 files belonging to 2 classes.\n",
      "Using 65 files for validation.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (160, 160)\n",
    "directory = \"dataset/\"\n",
    "train_dataset = image_dataset_from_directory(directory,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             subset='training',\n",
    "                                             seed=42)\n",
    "validation_dataset = image_dataset_from_directory(directory,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE,\n",
    "                                             validation_split=0.2,\n",
    "                                             subset='validation',\n",
    "                                             seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/f2.jpg\" style=\"width:500px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Fig. 1</b> </u><font color='purple'>  : <b>Some samples</b> <br> </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to plot some images as in Fig. 1. \n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_dataset.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocess and Augment Training Data\n",
    "\n",
    "`dataset.prefetch` is an important step in data preprocessing. \n",
    "\n",
    "Using `prefetch()` prevents a memory bottleneck that can occur when reading from disk. It sets aside some data and keeps it ready for when it's needed, by creating a source dataset from your input data, applying a transformation to preprocess it, then iterating over the dataset one element at a time. Because the iteration is streaming, the data doesn't need to fit into memory.\n",
    "\n",
    "`tf.data.experimental.AUTOTUNE` will choose the number of elements to prefetch automatically. Autotune prompts `tf.data` to tune that value dynamically at runtime, by tracking the time spent in each operation and feeding those times into an optimization algorithm. The optimization algorithm tries to find the best allocation of its CPU budget across all tunable operations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE  #AUTOTUNE= -1 ???\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data_augmenter\n",
    "\n",
    "To increase diversity in the training set and help your model learn the data better, it's standard practice to augment the images by transforming them, i.e., randomly flipping and rotating them. Keras offers a straightforward method for data augmentations, with built-in, customizable preprocessing layers. These layers are saved with the rest of your model and can be re-used later.\n",
    "\n",
    "Read the official docs for data augmentation [here](https://www.tensorflow.org/tutorials/images/data_augmentation).\n",
    "\n",
    "The function *data_augmenter* (for data augmentation) uses `Sequential` keras model composed of 2 layers:\n",
    "* `RandomFlip('horizontal')`\n",
    "* `RandomRotation(0.2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd6b3e9f32b1bf37",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def data_augmenter():\n",
    "    '''\n",
    "    Create a Sequential model composed of 2 layers\n",
    "    Returns:\n",
    "        tf.keras.Sequential\n",
    "    '''\n",
    "\n",
    "    data_augmentation = tf.keras.Sequential()\n",
    "    data_augmentation.add(RandomFlip(\"horizontal\"))\n",
    "    data_augmentation.add(RandomRotation(0.2))\n",
    "\n",
    "    \n",
    "    return data_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training images are augmented with simple transformations: see 9 variations from one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call data_augmenter() to create the augmentation object here\n",
    "\n",
    "data_augmentation = ?\n",
    "\n",
    "for image, _ in train_dataset.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image = image[0]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "        plt.imshow(augmented_image[0] / 255)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using MobileNetV2 for Transfer Learning \n",
    "\n",
    "MobileNetV2 was trained on ImageNet and is optimized to run on mobile and other low-power applications.MobileNetV2 is efficient for classification, object detection and image segmentation tasks and has 3 unique characteristics:\n",
    "\n",
    "*   Depthwise separable convolutions deal with both spatial and depth (number of channels) dimensions and provide lightweight feature filtering and creation.\n",
    "*   Input and output bottlenecks that preserve important information on either end of the block\n",
    "*   Shortcut connections between bottleneck layers\n",
    "\n",
    "\n",
    "### MobileNetV2 Convolutional Building Block\n",
    "<img src=\"images/mobilenetv2.png\" style=\"width:500px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Fig. 2</b> </u><font color='purple'>  : <b>MobileNetV2 Architecture</b> <br> This diagram was inspired by the original seen <a href=\"https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html#:~:text=MobileNetV2%20is%20a%20significant%20improvement,object%20detection%20and%20semantic%20segmentation.\">here</a>.</center></caption>\n",
    "\n",
    "MobileNetV2 uses depthwise separable convolutions as efficient building blocks. Traditional convolutions are often very resource-intensive, and  depthwise separable convolutions are able to reduce the number of trainable parameters and operations and also speed up convolutions in two steps: \n",
    "\n",
    "1. Depthwise convolution is the first step that computes an intermediate result by convolving on each of the channels independently. \n",
    "\n",
    "2. Pointwise convolution is the second step that is Shape of the depthwise convolution X Number of filters.\n",
    "\n",
    "Each block consists of a residual structure with a bottleneck at each end. These bottlenecks encode the intermediate inputs and outputs in a low dimensional space. \n",
    "\n",
    "The shortcut connections (similar to the ones in traditional residual networks) serve the same purpose of speeding up training and improving predictions. These connections skip over the intermediate convolutions and connect the bottleneck layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reuse the normalization values [-1,1] with which MobileNetV2 was pre-trained\n",
    "\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pretrained MobileNetV2 with the weights from ImageNet dataset by specifying `weights='imagenet'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = IMG_SIZE + (3,)  #(160,160,3)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=True,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model summary show all model's layers, the output shapes, the total number of parameters, trainable and non-trainable. \n",
    "The DepthwiseConv2D and SeparableConv2D layers are part of the intermediate expansion layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the model summary \n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many layers has the model \n",
    "nb_layers = ?\n",
    "\n",
    "#Get the names of the last 2 layers (called top layers),\n",
    "#responsible for the predictions (classification) in the model. \n",
    "print(base_model.layers[nb_layers - 2].name)\n",
    "print(base_model.layers[nb_layers - 1].name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the first batch of images from the tensorflow dataset, and run it through the MobileNetV2 base model to test out the model predictions on those images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "\n",
    "pred= base_model(image_batch)\n",
    "\n",
    "# Get the dimensions of image_batch, label_batch, pred \n",
    "# What are their meanings ?\n",
    "\n",
    "?\n",
    "\n",
    "#Try to plot one image from image_batch \n",
    "\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now decode the predictions made by the model.\n",
    "\n",
    "The predictions returned by the base model below follow the following format: i) class number, ii) human-readable label, iii) the probability of the image belonging to that class. top=2 means to return only the top two probabilities for that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "\n",
    "tf.keras.applications.mobilenet_v2.decode_predictions(pred.numpy(), top=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, none of the labels is \"alpaca.\"\n",
    "This is because MobileNet pretrained over ImageNet doesn't have labels for alpacas, so when you use the model, you get incorrectly classified images.\n",
    "\n",
    "Now, you will delete the top layer, which contains the ImageNet labels, and create a new binary classification layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Layer Freezing and model retraining \n",
    "\n",
    "You will use the pretrained model to modify the classifier task so that it's able to recognize alpacas: \n",
    "\n",
    "1. Delete the top layer (the classification layer): \n",
    "    * Set `include_top` in `base_model` as False\n",
    "2. Add a new top classification (prediction) layer: \n",
    "``Dense()``\n",
    "3. Freeze the base model and train only the newly-created classifier layer: \n",
    "    * Set `base_model.trainable=False` to avoid changing the weights and train *only* the new layer\n",
    "    * Set training in `base_model` to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpaca_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-106ac76f39286ee3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def alpaca_model(image_shape=IMG_SIZE, data_augmentation=data_augmenter()):\n",
    "    ''' Define a tf.keras model for binary classification out of \n",
    "    # MobileNetV2 model\n",
    "    Arguments:\n",
    "        image_shape -- Image width and height\n",
    "        data_augmentation -- data augmentation function\n",
    "    Returns:\n",
    "        tf.keras.model\n",
    "    '''\n",
    "        \n",
    "    #IMG_SIZE= (160, 160)\n",
    "    input_shape = image_shape + (3,)\n",
    "    \n",
    "    #Load the base pretrained MobileNetV2 model. \n",
    "    #Set include_top as False \n",
    "    base_model = ?\n",
    "    \n",
    "    # Freeze the base model by making it non trainable\n",
    "    base_model.trainable = ?\n",
    "    \n",
    "    # create the input layer (Same as the imageNetv2 input size)\n",
    "    inputs = Input(shape=input_shape) \n",
    "    \n",
    "    # apply data augmentation to the inputs\n",
    "    x = ?\n",
    "    \n",
    "# data preprocessing using the same weights as the model was trained on\n",
    "    x = preprocess_input(x) \n",
    "    \n",
    "    # set training to False\n",
    "    x = base_model(x, training=False) \n",
    "    \n",
    "    # Add new Binary classification layer\n",
    "    # use global average pooling to summarize the info in each channel\n",
    "    x = ?\n",
    "    \n",
    "    #include dropout with probability of 0.2 to avoid overfitting\n",
    "    x = ?\n",
    "        \n",
    "    # Add new Binary classification layer with one neuron\n",
    "    prediction_layer = ?\n",
    "    \n",
    "    outputs = prediction_layer(x) \n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new alpaca_model with data_augmentation\n",
    "model2 = ?\n",
    "\n",
    "## Get model summary, observe data augmentation layers \n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model2, with Adam optimizer & suggested base_learning_rate\n",
    "\n",
    "base_learning_rate = 0.01\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Different results when using loss='binary_crossentropy' !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model for 5 epochs\n",
    "initial_epochs = 5\n",
    "\n",
    "history = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy and loss curves \n",
    "\n",
    "acc = ?\n",
    "val_acc = ?\n",
    "\n",
    "loss = ? \n",
    "val_loss = ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fine-tuning the Model\n",
    "\n",
    "Try fine-tuning the model to improve accuracy. The way you achieve this is by unfreezing the layers at the end of the network, and then re-training the model on the final layers with a very low learning rate. Adapting the learning rate to go over these layers in smaller steps can yield more fine details - and higher accuracy.\n",
    "\n",
    "The intuition for what's happening: when the network is in its earlier stages, it trains on low-level features, like edges. In the later layers, more complex, high-level features like wispy hair or pointy ears begin to emerge. For transfer learning, the low-level features can be kept the same, as they have common features for most images. When you add new data, you generally want the high-level features to adapt to it, which is rather like letting the network learn to detect features more related to your data, such as soft fur or big teeth. \n",
    "\n",
    "To achieve this, unfreeze the final layers and re-run the optimizer with a smaller learning rate, keep all other layers frozen.\n",
    "\n",
    "Where the final layers actually begin is a bit arbitrary, so play around with this number. The important takeaway is that the later layers are the part of the network that contain the fine details (pointy ears, hairy tails) that are more specific to the particular problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5c3d1b52347cc066",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# unfreeze the base model \n",
    "base_model.trainable = ?\n",
    "\n",
    "# How many layers are in the base model ?\n",
    "print(\"Number of layers in the base model: ? \")\n",
    "?\n",
    "\n",
    "# Choose to fine-tune from this layer onwards\n",
    "fine_tune_at = ?\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = None\n",
    "\n",
    "    \n",
    "#Compile model2, with Adam optimizer and\n",
    "# learning rate = 0.1*base_learning_rate\n",
    "\n",
    "?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model for another few epochs, and see if the accuracy improved.\n",
    "\n",
    "fine_tune_epochs = 5\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model2.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training & validation accuracy and loss curves. You are expected to get something similar as in Fig. 3\n",
    "\n",
    "<img src=\"images/f3.jpg\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> <b>Fig. 3</b> </u><font color='purple'>  : <b>Some samples</b> <br> </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
